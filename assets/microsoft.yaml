- access:
    explanation: Microsoft does not provide VLMo to external researchers. One author
      commented that code would be pushed "soon" in [[November of 2021]](https://github.com/microsoft/unilm/issues/532),
      but the repository does not contain relevant changes.
    value: closed
  analysis: ''
  created_date:
    explanation: The date the model paper was released
    value: 2021-11-03
  dependencies:
  - Conceptual Captions
  - SBU Captions
  - COCO
  - Visual Genome
  - Wikipedia
  - BooksCorpus
  description: VLMo is a model for text-to-image generation
  feedback: ''
  intended_uses: ''
  license: none
  modality: text; image
  model_card: none
  monitoring: ''
  name: VLMo
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 562M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2111.02358
- access:
    explanation: Manual approval through early access request form required.
    value: limited
  analysis: ''
  created_date:
    explanation: Date model blog post was released
    value: 2022-09-28
  dependencies: []
  description: T-ULRv5 is a language model trained with two unique training objectives
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: T-ULRv5
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 2.2B parameters (dense)
  training_emissions: ''
  training_hardware: 256 A100
  training_time: Less than two weeks
  type: model
  url: https://www.microsoft.com/en-us/research/blog/microsoft-turing-universal-language-representation-model-t-ulrv5-tops-xtreme-leaderboard-and-trains-100x-faster/
- access:
    explanation: Manual approval through early access request form required.
    value: limited
  analysis: ''
  created_date:
    explanation: Date model blog post was released
    value: 2021-12-02
  dependencies: []
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Turing NLR-v5
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 5B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://www.microsoft.com/en-us/research/blog/efficiently-and-effectively-scaling-up-language-model-pretraining-for-best-language-representation-model-on-glue-and-superglue/?OCID=msr_blog_TNLRV5_tw
- access:
    explanation: Megatron-Turing NLG can be accessed through the [[Turing Academic
      Program]](https://www.microsoft.com/en-us/research/collaboration/microsoft-turing-academic-program/)
    value: limited
  analysis: ''
  created_date:
    explanation: 'The date of the Microsoft Research blog announcing MT-NLG [[Microsoft
      Research Blog]](https://www.microsoft.com/en-us/research/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/).

      '
    value: 2022-01-28
  dependencies:
  - The Pile
  description: 'Megatron-Turing NLG is a 530B parameter autoregressive language model.

    '
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Megatron-Turing NLG
  nationality: USA
  organization: Microsoft, NVIDIA
  prohibited_uses: ''
  quality_control: ''
  size: 530B parameters (dense)
  training_emissions: ''
  training_hardware: 4480 A100s (560 x 8)
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2201.11990
- access:
    explanation: Microsoft does not provide public access to VALL-E
    value: closed
  analysis: ''
  created_date:
    explanation: The date the [[model paper]](https://arxiv.org/abs/2301.02111) was
      released
    value: 2023-01-05
  dependencies: []
  description: Vall-E is a neural code model for text-to-speech synthesis
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; audio
  model_card: none
  monitoring: ''
  name: VALL-E
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: 16 V100 32GB GPUs
  training_time: ''
  type: model
  url: https://valle-demo.github.io/
- access:
    explanation: 'The feature is available to developers in a restricted technical
      preview [[GitHub CoPilot]](https://copilot.github.com/).

      '
    value: limited
  adaptation: unknown
  created_date:
    explanation: 'Date of the blog post introducing CoPilot [[GitHub Blog Post]] (https://github.blog/2021-06-29-introducing-github-copilot-ai-pair-programmer/).

      '
    value: 2021-06-29
  dependencies:
  - Codex
  description: 'GitHub CoPilot is a coding pair programmer assisting programmers as
    they write code.

    '
  failures: unknown
  feedback: 'Feedback can be provided in the CoPilot feedback project [[CoPilot feedback]]
    (https://github.com/github/feedback/discussions/categories/copilot-feedback).

    '
  intended_uses: 'GitHub CoPilot is intended to be used as a coding assistant.

    '
  license: unknown
  monitoring: "value: unknown explanation: >\n  There may be internal monitoring mechanisms\
    \ unknown to the public.\n"
  monthly_active_users: 'GitHub Copilot reportedly has over 1 million sign-ups [[Tweet
    Source]](https://twitter.com/sama/status/1539737789310259200?s=21&t=YPaYd0ZueJzrR6rLslUqzg).

    '
  name: GitHub CoPilot
  nationality: USA
  organization: Microsoft
  output_space: Code completions
  prohibited_uses: 'Access to GPT-3 is governed by GitHub Acceptable Use Policies
    and Terms of Service, both of which list a set of prohibited uses [[Use Policies]]
    (https://docs.github.com/en/site-policy/acceptable-use-policies/github-acceptable-use-policies)
    [[Terms of Service]] (https://docs.github.com/en/site-policy/github-terms/github-terms-of-service).

    '
  quality_control: 'GitHub is working on a filter to detect and suppress code generations
    that are verbatim from the training set [[GitHub Research Recitation]] (https://docs.github.com/en/github/copilot/research-recitation).
    According to the FAQ, GitHub implemented a simple filter that blocks emails in
    standard formats to protect personally identifiable data that may be present in
    the training data [[GitHub CoPilot]](https://copilot.github.com/).

    '
  terms_of_service: 'https://docs.github.com/en/site-policy/github-terms/github-terms-of-service

    '
  type: application
  url: https://copilot.github.com/
  user_distribution: unknown
- access: open
  analysis: ''
  created_date: 2022-09-24
  dependencies:
  - PubMed
  description: ''
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: text; text
  model_card: ''
  monitoring: ''
  name: BioGPT
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 1.5B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://academic.oup.com/bib/article/23/6/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9&login=true
- access: limited
  adaptation: unknown
  created_date: 2023-02-07
  dependencies:
  - ChatGPT API
  description: AI-powered Bing search engine and Edge browser, available in preview
    now at Bing.com, to deliver better search, more complete answers, a new chat experience
    and the ability to generate content. We think of these tools as an AI copilot
    for the web.
  failures: ''
  feedback: 'Feedback can be submitted at [bing.com](bing.com).

    '
  intended_uses: Search engine
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Bing Search
  nationality: USA
  organization: Microsoft
  output_space: Search results
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/
  user_distribution: ''
- access: closed
  analysis: ''
  created_date: 2023-03-01
  dependencies:
  - The Pile
  - CommonCrawl
  - LAION-2B-en
  - LAION-400M
  - COYO-700M
  - Conceptual Captions
  description: KOSMOS-1 is a multimodal language model that is capable of perceiving
    multimodal input, following instructions, and performing in-context learning for
    not only language tasks but also multimodal tasks.
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: image, text; image, text
  model_card: ''
  monitoring: ''
  name: KOSMOS-1
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 1.6B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2302.14045.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-07
  dependencies: []
  description: In the context of Bing, we have developed a proprietary way of working
    with the OpenAI model that allows us to best leverage its power. We call this
    collection of capabilities and techniques the Prometheus model. This combination
    gives you more relevant, timely and targeted results, with improved safety.
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: unknown
  model_card: ''
  monitoring: ''
  name: Prometheus
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/
- access: closed
  analysis: ''
  created_date: 2022-11-23
  dependencies:
  - FLD-900M
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; image
  model_card: ''
  monitoring: ''
  name: Florence
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 900M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2111.11432
- access: closed
  analysis: ''
  created_date: 2022-11-23
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: image, text
  monitoring: ''
  name: FLD-900M
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 900M image-text pairs
  type: dataset
  url: https://arxiv.org/abs/2111.11432
- access: limited
  adaptation: ''
  created_date: 2023-03-07
  dependencies:
  - Florence
  description: Cost-effective, production-ready computer vision services in Azure
    Cognitive Service for Vision. The improved Vision Services enables developers
    to create cutting-edge, market-ready, responsible computer vision applications
    across various industries.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license in the Microsoft Terms of Use govern the license
      rules for Azure services as outlined in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Azure Cognitive Services for Vision
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://azure.microsoft.com/en-us/support/legal/
  type: application
  url: https://azure.microsoft.com/en-us/blog/announcing-a-renaissance-in-computer-vision-ai-with-microsofts-florence-foundation-model/?utm_content=buffer16fa0&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer
  user_distribution: ''
- access: closed
  analysis: ''
  created_date: 2023-03-08
  dependencies:
  - OpenAI API
  description: ''
  feedback: ''
  intended_uses: ''
  license: none
  modality: text; image, text
  model_card: ''
  monitoring: ''
  name: VisualChatGPT
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2303.04671.pdf
- access: limited
  adaptation: ''
  created_date: 2023-03-16
  dependencies:
  - GPT-4 API
  description: It combines the power of language models with your data in the Microsoft
    Graph and the Microsoft 365 apps to turn your words into the most powerful productivity
    tool on the planet.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft 365 Copilot
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
  user_distribution: ''
- access: limited
  adaptation: ''
  created_date: 2023-03-16
  dependencies:
  - Microsoft 365 Copilot
  description: "Business Chat works across the langugae model, the Microsoft 365 apps,\
    \ and your data \u2014 your calendar, emails, chats, documents, meetings and contacts\
    \ \u2014 to do things you\u2019ve never been able to do before. You can give it\
    \ natural language prompts like \u201CTell my team how we updated the product\
    \ strategy,\u201D and it will generate a status update based on the morning\u2019\
    s meetings, emails and chat threads."
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft Business Chat
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://blogs.microsoft.com/blog/2023/03/16/introducing-microsoft-365-copilot-your-copilot-for-work/
  user_distribution: ''
- access: open
  adaptation: ''
  created_date: unknown
  dependencies:
  - Microsoft 365 Copilot
  description: Microsoft Excel is the industry leading spreadsheet software program,
    a powerful data visualization and analysis tool.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft Excel
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://www.microsoft.com/en-us/microsoft-365/excel
  user_distribution: ''
- access: open
  adaptation: ''
  created_date: unknown
  dependencies:
  - Microsoft 365 Copilot
  description: Microsoft Outlook is a personal information manager software system
    from Microsoft, available as a part of the Microsoft Office and Microsoft 365
    software suites.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft Outlook
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://www.microsoft.com/en-us/microsoft-365/outlook/email-and-calendar-software-microsoft-outlook
  user_distribution: ''
- access: limited
  adaptation: ''
  created_date: unknown
  dependencies:
  - Microsoft 365 Copilot
  description: Microsoft Power Platform is a line of business intelligence, app development,
    and app connectivity software applications.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft Power Platform
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://powerplatform.microsoft.com/en-us/
  user_distribution: ''
- access: open
  adaptation: ''
  created_date: unknown
  dependencies:
  - Microsoft 365 Copilot
  description: Microsoft PowerPoint empowers you to create clean slideshow presentations
    and intricate pitch decks and gives you a powerful presentation maker.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft PowerPoint
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://www.microsoft.com/en-us/microsoft-365/powerpoint
  user_distribution: ''
- access: open
  adaptation: ''
  created_date: unknown
  dependencies:
  - Microsoft 365 Copilot
  - Microsoft Business Chat
  description: Microsoft Teams is a proprietary business communication platform developed
    by Microsoft, as part of the Microsoft 365 family of products.
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft Teams
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://www.microsoft.com/en-us/microsoft-teams/group-chat-software
  user_distribution: ''
- access: open
  adaptation: ''
  created_date: unknown
  dependencies:
  - Microsoft 365 Copilot
  description: Microsoft Word is a word processing software developed by Microsoft
  failures: ''
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: ''
  name: Microsoft Word
  nationality: USA
  organization: Microsoft
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://www.microsoft.com/en-us/microsoft-365/word
  user_distribution: ''
- access: limited
  adaptation: unknown
  created_date: unknown
  dependencies: []
  description: 'Inside look is a Microsoft Office feature, composing document insights
    highlighting key points, expected time to read, and popularity among others.

    '
  failures: unknown
  feedback: unknown
  intended_uses: Providing document insights to users.
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: unknown
  monthly_active_users: unknown
  name: Microsoft Inside Look
  nationality: USA
  organization: Microsoft
  output_space: Document level insights for users.
  prohibited_uses: unknown
  quality_control: unknown
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://support.microsoft.com/en-us/office/see-file-insights-before-you-open-a-file-87a23bbc-a516-42e2-a7b6-0ecb8259e026
  user_distribution: unknown
- access: limited
  adaptation: unknown
  created_date: unknown
  dependencies: []
  description: 'Suggested replies is a Microsoft Outlook feature that suggests responses
    to emails, available in: English, Spanish, Italian, French, German, Portuguese
    Chinese Simplified, Chinese Traditional, Swedish, Russian, Korean, Czech, Hungarian,
    Arabic, Hebrew, Thai, Turkish, Japanese, Dutch, Norwegian, Danish, and Polish.

    '
  failures: unknown
  feedback: unknown
  intended_uses: Suggesting email replies.
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: unknown
  monthly_active_users: unknown
  name: Microsoft Suggested Replies
  nationality: USA
  organization: Microsoft
  output_space: Suggested emails.
  prohibited_uses: unknown
  quality_control: unknown
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://support.microsoft.com/en-us/office/use-suggested-replies-in-outlook-19316194-0434-43ba-a742-6b5890157379
  user_distribution: unknown
- access: limited
  adaptation:
    explanation: See [[blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: Security Copilot combines OpenAI's GPT-4 generative AI with a security-specific
      model from Microsoft. This security-specific model in turn incorporates a growing
      set of security-specific skills and is informed by Microsoft's unique global
      threat intelligence and more than 65 trillion daily signals.
  created_date:
    explanation: The date Security Copilot was announced in the [[Microsoft blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: 2023-03-28
  dependencies:
  - GPT-4
  - Microsoft security-specific model
  description: 'Microsoft Security Copilot is an AI-powered security analysis tool
    that enables analysts to respond to threats quickly, process signals at machine
    speed, and assess risk exposure in minutes.

    '
  failures: unknown
  feedback: unknown
  intended_uses: Security Copilot is designed to enhance the capabilities of cybersecurity
    professionals. It leverages machine speed and scale to accelerate response to
    security incidents, discover and process threat signals, and assess risk exposure
    within minutes.
  license:
    explanation: 'Software license as described in the Terms of Service document.

      '
    value: custom
  monitoring: ''
  monthly_active_users: unknown
  name: Microsoft Security Copilot
  nationality: USA
  organization: Microsoft
  output_space:
    explanation: See [[product demo]](https://www.microsoft.com/en-us/security/business/ai-machine-learning/microsoft-security-copilot).
    value: Actionable responses to security-related questions (text and image). Security
      event, incident or threat reports (PowerPoint slide).
  prohibited_uses: unknown
  quality_control:
    explanation: See [[blog post]](https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/).
    value: Security Copilot employs a closed-loop learning system that learns from
      user interactions and feedback, enabling it to provide more coherent, relevant,
      and useful answers that continually improve over time. Security Copilot is committed
      to delivering safe, secure, and responsible AI solutions, ensuring that customers'
      data and AI models are protected with enterprise compliance and security controls.
      Customer data is owned and controlled by them, and not used to train AI models
      for anyone outside their organization.
  terms_of_service: https://www.microsoft.com/legal/terms-of-use
  type: application
  url: https://blogs.microsoft.com/blog/2023/03/28/introducing-microsoft-security-copilot-empowering-defenders-at-the-speed-of-ai/
  user_distribution: unknown
- access: open
  analysis: Evaluated on GLUE, SQuAD 2.0, and CoQA benchmarks.
  created_date: 2019-10-01
  dependencies: []
  description: UniLM is a unified language model that can be fine-tuned for both natural
    language understanding and generation tasks.
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: text; text
  model_card: ''
  monitoring: ''
  name: UniLM
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 340M parameters (dense)
  training_emissions: unknown
  training_hardware: 8 NVIDIA Tesla V100 32GB GPUs
  training_time: 10,000 steps in 7 hours
  type: model
  url: https://proceedings.neurips.cc/paper_files/paper/2019/file/c20bb2d9a50d5ac1f713f8b34d9aac5a-Paper.pdf
- access: limited
  analysis: ''
  created_date: 2021-04-12
  dependencies: []
  description: Docugami is a LLM focused on writing business documents and data using
    generative AI.
  feedback: ''
  intended_uses: analyzing, writing, and connecting business documents and data
  license: ''
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Docugami
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 20B parameters (dense)
  training_emissions: unknown
  training_hardware: ''
  training_time: unknown
  type: model
  url: https://www.docugami.com/generative-ai
- access: open
  analysis: Evaluated on a range of standardized vision benchmarks, and achieves state
    of the art performance on all experimentally.
  created_date: 2022-08-31
  dependencies:
  - Multiway Transformer network
  description: BEiT-3 is a general-purpose multimodal foundation model for vision
    and vision-language tasks.
  feedback: ''
  intended_uses: ''
  license: ''
  modality: image, text; image, text
  model_card: ''
  monitoring: ''
  name: BEiT-3
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 1.9B parameters (dense)
  training_emissions: unknown
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2208.10442.pdf
- access: open
  analysis: Reports results on standard LLM benchmarks in comparison to other LLMs
    and test sets.
  created_date: 2023-04-24
  dependencies:
  - LLaMA
  - Evol-Instruct
  - Alpaca dataset
  description: Starting with an initial set of instructions, we use our proposed Evol-Instruct
    to rewrite them step by step into more complex instructions. Then, we mix all
    generated instruction data to fine-tune LLaMA. We call the resulting model WizardLM.
  feedback: https://huggingface.co/datasets/WizardLM/evol_instruct_70k/discussions
  intended_uses: Creating large amounts of instruction data, particularly with high
    complexity
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/WizardLM/WizardLM-13B-1.0
  monitoring: ''
  name: WizardLM
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 7B parameters (dense)
  training_emissions: ''
  training_hardware: 8 V100 GPUs
  training_time: 70 hours on 3 epochs
  type: model
  url: https://arxiv.org/pdf/2304.12244v1.pdf
- access: open
  analysis: Evaluated on four prominent code generation benchmarks HumanEval, HumanEval+,
    MBPP, and DS100.
  created_date: 2023-08-26
  dependencies:
  - Evol-Instruct
  - Alpaca dataset
  - StarCoder
  description:
    explanation: WizardCoder refers to both a set of models fine-tuned on StarCoder
      and a set of models fined-tuned on Code LLaMA. This node refers to the set fine-tuned
      on StarCoder.
    value: WizardCoder empowers Code LLMs with complex instruction fine-tuning, by
      adapting the Evol-Instruct method to the domain of code.
  feedback: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0/discussions
  intended_uses: ''
  license:
    explanation: Model license can be found at https://github.com/nlpxucan/WizardLM/blob/main/WizardCoder/MODEL_WEIGHTS_LICENSE.
      Code license is under Apache 2.0
    value: BigCode Open Rail-M
  modality:
    explanation: text; text
    value: text; text
  model_card: https://huggingface.co/WizardLM/WizardCoder-Python-34B-V1.0
  monitoring: ''
  name: WizardCoder
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 34B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2306.08568.pdf
- access: closed
  analysis: Evaluated on standard image processing benchmarks
  created_date: 2023-11-10
  dependencies:
  - FLD-5B
  description: WizardCoder empowers Code LLMs with complex instruction fine-tuning,
    by adapting the Evol-Instruct method to the domain of code.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: image, text; text
  model_card: none
  monitoring: ''
  name: Florence-2
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 771M parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2311.06242.pdf
- access: closed
  analysis: FLD-5B evaluated in comparison to datasets that power other large-scale
    image models on standard image benchmarks.
  created_date: 2023-11-10
  datasheet: ''
  dependencies: []
  description: FLD-5B is the dataset that powers Florence-2
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: image, text
  monitoring: ''
  name: FLD-5B
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 1.3B image-text annotations
  type: dataset
  url: https://arxiv.org/pdf/2311.06242.pdf
- access: open
  analysis: Models trained on OpenOrca compared to GPT-series on language benchmarks.
  created_date: 2023-06-05
  datasheet: https://huggingface.co/datasets/Open-Orca/OpenOrca
  dependencies:
  - GPT-3.5
  - GPT-4
  - Flan Collection
  description: The OpenOrca dataset is a collection of augmented FLAN Collection data.
    Currently ~1M GPT-4 completions, and ~3.2M GPT-3.5 completions. It is tabularized
    in alignment with the distributions presented in the ORCA paper and currently
    represents a partial completion of the full intended dataset, with ongoing generation
    to expand its scope.
  excluded: ''
  feedback: none
  included: ''
  intended_uses: training and evaluation in the field of natural language processing.
  license: MIT
  modality: text
  monitoring: ''
  name: OpenOrca
  nationality: USA
  organization: Microsoft
  prohibited_uses: none
  quality_control: ''
  sample: []
  size: 4.5M text queries
  type: dataset
  url: https://huggingface.co/datasets/Open-Orca/OpenOrca
- access: open
  analysis: LlongOrca evaluated on BigBench-Hard and AGIEval results.
  created_date: 2023-08-01
  dependencies:
  - OpenOrca
  - LLongMA-2
  description: LlongOrca is an attempt to make OpenOrca able to function in a Llong
    context.
  feedback: https://huggingface.co/Open-Orca/LlongOrca-7B-16k/discussions
  intended_uses: training and evaluation in the field of natural language processing.
  license: LLaMA 2
  modality: text; text
  model_card: https://huggingface.co/Open-Orca/LlongOrca-7B-16k
  monitoring: ''
  name: LlongOrca
  nationality: USA
  organization: Microsoft
  prohibited_uses: none
  quality_control: ''
  size: 7B parameters (dense)
  training_emissions: unknown
  training_hardware: 8x A6000-48GB (first-gen) GPUs
  training_time: 37 hours
  type: model
  url: https://huggingface.co/Open-Orca/LlongOrca-7B-16k
- access: open
  analysis: Evaluated on common sense reasoning, language understanding, and multi-step
    reasoning compared to other SOTA language models.
  created_date: 2023-09-11
  dependencies:
  - phi-1
  description: Phi-1.5 is a large language transformer model.
  feedback: https://huggingface.co/microsoft/phi-1_5/discussions
  intended_uses: Phi-1.5 is best suited for answering prompts using the QA format,
    the chat format, and the code format.
  license:
    explanation: can be found via the license tab at top of https://huggingface.co/microsoft/phi-1_5
    value: MIT
  modality: text; text
  model_card: https://huggingface.co/microsoft/phi-1_5
  monitoring: none
  name: Phi-1.5
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: generic web-crawl data is removed from dataset.
  size: 1.3B parameters (dense)
  training_emissions: unknown
  training_hardware: 32 A100-40G GPUs
  training_time: 8 days
  type: model
  url: https://arxiv.org/pdf/2309.05463.pdf
- access: open
  analysis: Orca 2 has been evaluated on a large number of tasks ranging from reasoning
    to grounding and safety.
  created_date: 2023-11-21
  dependencies:
  - LLaMA 2
  description: Orca 2 is a finetuned version of LLAMA-2 for research purposes.
  feedback: https://huggingface.co/microsoft/Orca-2-13b/discussions
  intended_uses: Orca 2 is built for research purposes only. The main purpose is to
    allow the research community to assess its abilities and to provide a foundation
    for building better frontier models.
  license:
    explanation: can be found at https://huggingface.co/microsoft/Orca-2-13b/blob/main/LICENSE
    value: custom
  modality: text; text
  model_card: https://huggingface.co/microsoft/Orca-2-13b
  monitoring: unknown
  name: Orca 2
  nationality: USA
  organization: Microsoft
  prohibited_uses: Any purposes other than research.
  quality_control: ''
  size: 13B parameters (dense)
  training_emissions: unknown
  training_hardware: 32 NVIDIA A100 80GB GPUs
  training_time: 80 hours
  type: model
  url: https://arxiv.org/pdf/2311.11045.pdf
- access: open
  analysis: The model has been evaluated against benchmarks that test common sense,
    language understanding, mathematics, coding, long-term context, and logical reasoning.
    The Phi-3 Medium-128K-Instruct demonstrated robust and state-of-the-art performance.
  created_date: 2024-05-21
  dependencies: []
  description: Phi-3 is a 14 billion-parameter, lightweight, state-of-the-art open
    model trained using the Phi-3 datasets.
  feedback: https://huggingface.co/microsoft/Phi-3-medium-128k-instruct/discussions
  intended_uses: The model's primary use cases are for commercial and research purposes
    that require capable reasoning in memory or compute constrained environments and
    latency-bound scenarios. It can also serve as a building block for generative
    AI-powered features.
  license: MIT
  modality: text; text
  model_card: https://huggingface.co/microsoft/Phi-3-medium-128k-instruct
  monitoring: Issues like allocation, high-risk scenarios, misinformation, generation
    of harmful content and misuse should be monitored and addressed.
  name: Phi-3
  nationality: USA
  organization: Microsoft
  prohibited_uses: The model should not be used for high-risk scenarios without adequate
    evaluation and mitigation techniques for accuracy, safety, and fairness.
  quality_control: The model underwent post-training processes viz. supervised fine-tuning
    and direct preference optimization to increase its capability in following instructions
    and aligning to safety measures.
  size: 14B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/abs/2404.14219
- access: closed
  analysis: Evaluated by comparing climate predictions to actual happened events.
  created_date: 2024-05-28
  dependencies: []
  description: Aurora is a large-scale foundation model of the atmosphere trained
    on over a million hours of diverse weather and climate data.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: text; climate forecasts
  model_card: none
  monitoring: ''
  name: Aurora
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: 1.3B parameters
  training_emissions: unknown
  training_hardware: 32 A100 GPUs
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2405.13063
- access: closed
  analysis: Evaluated on a digital pathology benchmark comprising 9 cancer subtyping
    tasks and 17 pathomics tasks, with Prov-GigaPath demonstrating SoTA performance
    in 25 out of 26 tasks.
  created_date: 2024-05-22
  dependencies:
  - GigaPath
  description: "Prov-GigaPath is a whole-slide pathology foundation model pretrained\
    \ on 1.3 billion 256\u2009\xD7\u2009256 pathology image tiles."
  feedback: none
  intended_uses: ''
  license: unknown
  modality: image; embeddings
  model_card: none
  monitoring: ''
  name: Prov-GigaPath
  nationality: USA
  organization: Microsoft
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: 4 80GB A100 GPUs
  training_time: 2 days
  type: model
  url: https://www.nature.com/articles/s41586-024-07441-w
- access: open
  analysis: The model was evaluated across a variety of public benchmarks, comparing
    with a set of models including Mistral-Nemo-12B-instruct-2407, Llama-3.1-8B-instruct,
    Gemma-2-9b-It, Gemini-1.5-Flash, and GPT-4o-mini-2024-07-18. It achieved a similar
    level of language understanding and math as much larger models. It also displayed
    superior performance in reasoning capability, even with only 6.6B active parameters.
    It was also evaluated for multilingual tasks.
  created_date: 2024-09-08
  dependencies:
  - Phi-3 dataset
  description: Phi-3.5-MoE is a lightweight, state-of-the-art open model built upon
    datasets used for Phi-3 - synthetic data and filtered publicly available documents,
    with a focus on very high-quality, reasoning dense data. It supports multilingual
    and has a 128K context length in tokens. The model underwent a rigorous enhancement
    process, incorporating supervised fine-tuning, proximal policy optimization, and
    direct preference optimization to ensure instruction adherence and robust safety
    measures.
  feedback: Unknown
  intended_uses: The model is intended for commercial and research use in multiple
    languages. It is designed to accelerate research on language and multimodal models,
    and for use as a building block for generative AI powered features. It is suitable
    for general purpose AI systems and applications which require memory/computed
    constrained environments, latency bound scenarios, and strong reasoning.
  license: MIT
  modality: text; text
  model_card: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct
  monitoring: Unknown
  name: Phi-3.5-MoE
  nationality: USA
  organization: Microsoft
  prohibited_uses: The model should not be used for downstream purposes it was not
    specifically designed or evaluated for. Developers should evaluate and mitigate
    for accuracy, safety, and fariness before using within a specific downstream use
    case, particularly for high risk scenarios.
  quality_control: The model was enhanced through supervised fine-tuning, proximal
    policy optimization, and direct preference optimization processes for safety measures.
  size: 61B parameters (sparse); 6.6B active parameters
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://huggingface.co/microsoft/Phi-3.5-MoE-instruct
- access:
    explanation: Phi-4 is available on Azure AI Foundry and on Hugging Face.
    value: open
  analysis: Phi-4 outperforms comparable and larger models on math related reasoning.
  created_date: 2024-12-13
  dependencies: []
  description: the latest small language model in Phi family, that offers high quality
    results at a small size (14B parameters).
  feedback: unknown
  intended_uses: Specialized in complex reasoning, particularly good at math problems
    and high-quality language processing.
  license: unknown
  modality:
    explanation: Today we are introducing Phi-4 , our 14B parameter state-of-the-art
      small language model (SLM) that excels at complex reasoning in areas such as
      math, in addition to conventional language processing.
    value: text; text
  model_card: unknown
  monitoring: Azure AI evaluations in AI Foundry enable developers to iteratively
    assess the quality and safety of models and applications using built-in and custom
    metrics to inform mitigations.
  name: Phi-4
  nationality: USA
  organization: Microsoft
  prohibited_uses: unknown
  quality_control: Building AI solutions responsibly is at the core of AI development
    at Microsoft. We have made our robust responsible AI capabilities available to
    customers building with Phi models.
  size:
    explanation: a small size (14B parameters).
    value: 14B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://techcommunity.microsoft.com/blog/aiplatformblog/introducing-phi-4-microsoft%E2%80%99s-newest-small-language-model-specializing-in-comple/4357090
