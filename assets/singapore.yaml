- access: open
  analysis: Performance assessed on BIG-bench arithmetic sub-task, and various elementary
    arithmetic tasks.
  created_date: 2023-05-23
  dependencies:
  - LLaMA
  - GOAT dataset
  description: GOAT is a fine-tuned LLaMA model which uses the tokenization of numbers
    to significantly outperform benchmark standards on a range of arithmetic tasks.
  feedback: ''
  intended_uses: Integration into other instruction-tuned LLMs to further enhance
    arithmetic reasoning abilities in solving math word problems.
  license: Apache 2.0
  modality: text; text
  model_card: none
  monitoring: ''
  name: GOAT
  nationality: Singapore
  organization: National University of Singapore
  prohibited_uses: ''
  quality_control: Number data is randomly generated from log space to reduce likelihood
    of redundancy and range of magnitudes.
  size: 7B parameters (dense)
  training_emissions: unknown
  training_hardware: 24 GB VRAM GPU
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2305.14201.pdf
- access: open
  analysis: Evaluated on relatively simple established benchmarks.
  created_date: 2024-01-12
  dependencies:
  - RedPajama
  - The Stack
  description: OpenMoE is a series of fully open-sourced and reproducible decoder-only
    MoE LLMs.
  feedback: https://huggingface.co/OrionZheng/openmoe-base/discussions
  intended_uses: ''
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/OrionZheng/openmoe-base
  monitoring: unknown
  name: OpenMoE
  nationality: unknown
  organization: National University of Singapore, University of Edinburgh, ETH Zurich
  prohibited_uses: ''
  quality_control: unknown
  size: 34B parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://github.com/XueFuzhao/OpenMoE
