- access: open
  analysis: ''
  created_date: 2022-12-15
  dependencies:
  - The Pile
  description: ''
  feedback: ''
  intended_uses: ''
  license: bigscience-bloom-rail-1.0
  modality: text; text
  model_card: ''
  monitoring: ''
  name: BioMedLM
  nationality: USA
  organization: Stanford
  prohibited_uses: ''
  quality_control: ''
  size: 2.7B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://crfm.stanford.edu/2022/12/15/pubmedgpt.html
- access: open
  analysis: Evaluated on own framework that tests domain-specific tasks in medical
    field.
  created_date: 2022-11-23
  dependencies:
  - Stable Diffusion
  - RoentGen radiology dataset
  description: RoentGen is a generative medical imaging model that can create visually
    convincing X-ray images.
  feedback: ''
  intended_uses: ''
  license: ''
  modality: text; image
  model_card: ''
  monitoring: ''
  name: RoentGen
  nationality: USA
  organization: Stanford
  prohibited_uses: ''
  quality_control: ''
  size: 330M parameters (dense)
  training_emissions: unknown
  training_hardware: 64 A100 GPUs
  training_time: 60k training steps per day
  type: model
  url: https://arxiv.org/pdf/2211.12737.pdf
- access: open
  analysis: Evaluated on three physical control tasks, drawing, steering, and human
    body movement on various dynamics
  created_date: 2023-06-12
  dependencies:
  - GPT-2
  - BABEL
  - text-davinci-003
  description: Model trained to generate language corrections for physical control
    tasks.
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: human trajectories; text
  model_card: ''
  monitoring: ''
  name: CORGI
  nationality: USA
  organization: Stanford
  prohibited_uses: ''
  quality_control: ''
  size: 124M parameters (dense)
  training_emissions: ''
  training_hardware: one NVIDIA A40 GPU
  training_time:
    explanation: The authors do not report the training time, but do report that they
      train for 200 epochs.
    value: unknown
  type: model
  url: https://arxiv.org/pdf/2306.07012.pdf
- access:
    explanation: The dataset can be downloaded from [[Hugging Face]](https://huggingface.co/datasets/tatsu-lab/alpaca).
      The code for generating data is available on the [[GitHub repository]](https://github.com/tatsu-lab/stanford_alpaca#data-generation-process).
    value: open
  analysis: ''
  created_date:
    explanation: 'The date the [[blog post]](https://crfm.stanford.edu/2023/03/13/alpaca.html)
      was released.

      '
    value: 2023-03-13
  datasheet: https://huggingface.co/datasets/tatsu-lab/alpaca
  dependencies:
  - text-davinci-003
  description: 'Alpaca dataset consistes of 52,000 instruction-following demonstrations
    generated in the style of the [Self-Instruct framework](https://github.com/yizhongw/self-instruct)
    using OpenAI''s text-davinci-003 engine. This instruction data can be used to
    conduct instruction-tuning for language models and make the language model follow
    instruction better.

    '
  excluded: ''
  feedback: Feedback can be provided on [[GitHub Issues]](https://github.com/tatsu-lab/stanford_alpaca/issues).
  included: ''
  intended_uses: Alpaca is intended and licensed for research use only.
  license: CC BY-NC 4.0
  modality: text (English)
  monitoring: ''
  name: Alpaca dataset
  nationality: USA
  organization: Stanford
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 52K instruction-following demonstrations
  type: dataset
  url: https://crfm.stanford.edu/2023/03/13/alpaca.html
- access:
    explanation: The weight diff between Alpaca-7B and LLaMA-7B is located on the
      [[Hugging Face]](https://huggingface.co/tatsu-lab/alpaca-7b-wdiff). To recover
      the original Alpaca-7B weights, follow the steps given [[here]](https://github.com/tatsu-lab
      stanford_alpaca#recovering-alpaca-weights). Training and data generation code
      can be found on the [[GitHub repository]](https://github.com/tatsu-lab/stanford_alpaca).
      An [[online demo]](https://chat.lmsys.org/?model=alpaca-13b) is also available.
    value: open
  analysis: ''
  created_date:
    explanation: 'The date the [[blog post]](https://crfm.stanford.edu/2023/03/13/alpaca.html)
      was released.

      '
    value: 2023-03-13
  dependencies:
  - LLaMa
  - Alpaca dataset
  description: 'Alpaca-7B is an instruction-following model fine-tuned from the LLaMA
    7B model on 52K instruction-following demonstrations.

    '
  feedback: Feedback can be provided on [[GitHub Issues]](https://github.com/tatsu-lab/stanford_alpaca/issues).
  intended_uses: Alpaca is intended and licensed for research use only.
  license: CC BY NC 4.0 (model weights)
  modality: text (English)
  model_card: ''
  monitoring: ''
  name: Alpaca
  nationality: USA
  organization: Stanford
  prohibited_uses: ''
  quality_control: ''
  size: 7B parameters (dense model)
  training_emissions: unknown
  training_hardware: ''
  training_time: ''
  type: model
  url: https://crfm.stanford.edu/2023/03/13/alpaca.html
- access: open
  analysis: Merlin has been comprehensively evaluated on 6 task types and 752 individual
    tasks. The non-adapted (off-the-shelf) tasks include zero-shot findings classification,
    phenotype classification, and zero-shot cross-modal retrieval, while model adapted
    tasks include 5-year chronic disease prediction, radiology report generation,
    and 3D semantic segmentation. It has undergone internal validation on a test set
    of 5,137 CTs, and external validation on 7,000 clinical CTs and on two public
    CT datasets (VerSe, TotalSegmentator).
  created_date: 2024-09-08
  dependencies: []
  description: Merlin is a 3D Vision Language Model that's designed for interpretation
    of abdominal computed tomography (CT) scans. It uses both structured Electronic
    Health Record (EHR) and unstructured radiology reports for supervision without
    requiring additional manual annotations. The model was trained on a high-quality
    clinical dataset of paired CT scans, EHR diagnosis codes, and radiology reports
    and was evaluated on 6 task types and 752 individual tasks.
  feedback: Feedback and reports for problems with the model should likely be routed
    to Stanford Center for Artificial Intelligence in Medicine and Imaging, or the
    corresponding author of the research (louis.blankemeier@stanford.edu).
  intended_uses: This model is intended for use in the interpretation of abdominal
    computed tomography (CT) scans, chronic disease prediction, radiology report generation,
    and 3D semantic segmentation.
  license: Unknown
  modality: image; text
  model_card: unknown
  monitoring: Unknown
  name: Merlin
  nationality: unknown
  organization: Stanford Center for Artificial Intelligence in Medicine and Imaging,
    Stanford University
  prohibited_uses: The model should not be used outside of healthcare-related context,
    such as for personal or non-medical commercial purposes.
  quality_control: The model has undergone extensive evaluations and also internal
    and external validation tests.
  size: Unknown
  training_emissions: Unknown
  training_hardware: Single GPU.
  training_time: Unknown
  type: model
  url: https://arxiv.org/pdf/2406.06512
