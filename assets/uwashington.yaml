- access: open
  analysis: Reports results on the Vicuna benchmark and compares performance level
    and time expenditure with ChatGPT
  created_date: 2023-05-23
  dependencies:
  - QLoRA
  - OASST1
  description: Guanaco is a model family trained with QLORA, an efficient finetuning
    approach that reduces memory usage enough to finetune a 65B parameter model on
    a single 48GB GPU while preserving full 16-bit finetuning task performance.
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Guanaco
  nationality: USA
  organization: University of Washington
  prohibited_uses: ''
  quality_control: ''
  size: 33B parameters (dense)
  training_emissions: ''
  training_hardware: A single 24 GB GPU
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2305.14314v1.pdf
- access: open
  analysis: Evaluated on benchmark music understanding tasks on SOTA music datasets.
  created_date: 2023-10-11
  dependencies:
  - LLaMA 2
  - Jukebox
  description: Llark is an instruction-tuned multimodal model for music understanding.
  feedback: none
  intended_uses: ''
  license: Apache 2.0
  modality: audio, text; text
  model_card: none
  monitoring: ''
  name: Llark
  nationality: USA
  organization: University of Washington, Spotify
  prohibited_uses: ''
  quality_control: ''
  size: 12B parameters (dense)
  training_emissions: unknown
  training_hardware: 4 80GB NVIDIA A40 GPUs
  training_time: 54 hours
  type: model
  url: https://arxiv.org/pdf/2310.07160.pdf
