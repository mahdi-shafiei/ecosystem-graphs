- access:
    explanation: https://huggingface.co/datasets/c4
    value: open
  analysis: https://arxiv.org/abs/2104.08758
  created_date:
    explanation: The date the T5 paper was made public.
    value: 2019-10-23
  datasheet: https://huggingface.co/datasets/c4
  dependencies:
  - CommonCrawl
  description: The Colossal Clean Crawled Corpus (C4) is a processed version of Common
    Crawl to facilitate transfer learning in NLP.
  excluded: 'Data was filtered for English using langdetect. Further, data was filtered
    to end in terminal punctuation, to remove short pages (less than 5 sentences),
    and to remove "Dirty, Naughty, Obscene or Otherwise Bad Words".

    '
  feedback: https://huggingface.co/datasets/c4/discussions
  included: none
  intended_uses: To faciliate transfer learning research in NLP.
  license: ODC-By 1.0
  modality: text
  monitoring: none
  name: C4
  nationality: USA
  organization: Google
  prohibited_uses: none
  quality_control: 'Data filtering excluded obscene words from a block list as well
    as short documents and some deduplication was done based on string overlap.

    '
  sample:
  - https://huggingface.co/datasets/c4/viewer/en/train
  size: 750GB
  type: dataset
  url: https://arxiv.org/abs/1910.10683
- access: closed
  analysis: unknown
  created_date:
    explanation: 'The date of the Google product update blog announcing that BERT
      models were for ranking and featured snippets in Search.

      '
    value: 2019-11-25
  datasheet: none
  dependencies: []
  description: 'The dataset used to train Internal Google BERT models.

    '
  excluded: unknown
  feedback: unknown
  included:
    explanation: 'Although we don''t exactly know the contents of the Internal Google
      BERT dataset, it likely includes contents from web pages and search queries.

      '
    value: Web pages, and search queries
  intended_uses:
    explanation: 'We don''t have an exhaustive list of the intended use cases for
      the Internal Google BERT dataset, but we know that BERT was used in Google Search.

      '
    value: unknown
  license: unknown
  modality: text
  monitoring: unknown
  name: Internal Google BERT dataset
  nationality: USA
  organization: Google
  prohibited_uses: unknown
  quality_control: unknown
  sample: []
  size: unknown
  type: dataset
  url: https://blog.google/products/search/search-language-understanding-bert/
- access:
    explanation: Dataset can be downloaded at [[Download]](https://ai.google.com/research/ConceptualCaptions/download)
    value: open
  analysis:
    explanation: See [[Experimental Results]](https://aclanthology.org/P18-1238.pdf#section.5)
    value: Authors evaluate the dataset on two image captioning models - RNN-based
      model and Transformer model, under two experimental conditions - using the training
      & development sets provided by the MS COCO dataset, versus training & development
      sets using the Conceptual dataset. They use three different test sets- the blind
      COCO-C40 test set, the Conceptual test set and the Flickr 1K test set. They
      present both Human and Automatic evaluation results. Human evaluations indicate
      that the Conceptual-based models are superior. Automatic models fail to corroborate
      the human evaluation results. This highlights the weakness of these automatic
      metrics.
  created_date:
    explanation: Due to the lack of information about the exact date, it is assumed
      to be the 1st of the known month of creation.
    value: 2018-07-01
  datasheet: none
  dependencies: []
  description: 'A dataset containing 3 million (image-URL, caption) pairs designed
    for the training and evaluation of machine learned image captioning systems.

    '
  excluded:
    explanation: See [[Conceptual Captions Dataset Creation]](https://aclanthology.org/P18-1238.pdf#section.3)
    value: "The following filtering steps are applied in the given order:\n1. Image-based\
      \ Filtering - \"It only keeps JPEG images where both dimensions are greater\
      \ than 400 pixels, and the ratio of larger to smaller dimension is no more than\
      \ 2. It excludes images that trigger pornography or profanity detectors. These\
      \ filters discard more than 65% of the candidates.\"\n2. Text-based Filtering\
      \ - \"Candidates with no determiner, no noun, or no preposition are discarded;\
      \ candidates with a high noun ratio are also discarded; candidates with a high\
      \ rate of token repetition are discarded; candidates where the first word is\
      \ not capitalized, or with too high capitalized-word ratio are discarded; we\
      \ use a vocabulary VW of 1B token types, appearing at least 5 times in the English\
      \ Wikipedia, and discard candidates that contain tokens that are not found in\
      \ this vocabulary. candidates that score too high or too low on the polarity\
      \ annotations, or trigger the pornography/profanity detectors, are discarded;\
      \ predefined boiler-plate prefix/suffix sequences matching the text are cropped,\
      \ e.g. \u201Cclick to enlarge picture\u201D, \u201Cstock photo\u201D; we also\
      \ drop text which begins/ends in certain patterns, e.g. \u201Cembedded image\
      \ permalink\u201D, \u201Cprofile photo\u201D. These filters only allow around\
      \ 3% of the incoming candidates to pass to the later stages.\"\n3. Image&Text-based\
      \ Filtering - \"We filter out candidates for which none of the text tokens can\
      \ be mapped to the content of the image. This filter discards around 60% of\
      \ the incoming candidates.\"\n4. Text Transformation with Hypernymization -\
      \ \"Noun modifiers of certain types (proper nouns, numbers, units) are removed;\
      \ dates, durations, and preposition-based locations (e.g., \"in Los Angeles\"\
      ) are removed; named-entities are identified, matched against the KG entries,\
      \ and substitute with their hypernym; resulting coordination noun-phrases with\
      \ the same head (e.g., \"actor and actor\") are resolved into a single-head,\
      \ pluralized form (e.g., \"actors\"). Around 20% of samples are discarded during\
      \ this transformation. We then cluster all resolved entities (e.g., 2560 \"\
      actor\", \"dog\", \"neighborhood\", etc.) and keep only the candidates for which\
      \ all detected types have a count of over 100 (around 55% of the candidates).\"\
      \n"
  feedback: Feedback can be provided by creating an issue in the [[Conceptual Captions
    GitHub repository]](https://github.com/google-research-datasets/conceptual-captions)
    or by emailing at conceptual-captions at google.com
  included: ''
  intended_uses: ''
  license:
    explanation: '[[Conceptual Captions License]](https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)

      '
    value: Conceptual Captions License
  modality: image, text
  monitoring: unknown
  name: Conceptual Captions
  nationality: USA
  organization: Google
  prohibited_uses: unknown
  quality_control:
    explanation: See [[Conceptual Captions Dataset Creation]](https://aclanthology.org/P18-1238.pdf#section.3)
    value: Input candidate (image, caption) pairs pass through several stages of filtering
      and processing to ensure quality.
  sample: []
  size: 3.3M (image, text) pairs
  type: dataset
  url: https://aclanthology.org/P18-1238/
- access:
    explanation: Dataset is available at [[Conceptual 12M GitHub repository]](https://github.com/google-research-datasets/conceptual-12m).
    value: open
  analysis:
    explanation: See [[Evaluating Vision-and-Language PreTraining Data]](https://arxiv.org/pdf/2102.08981.pdf#section.3)
    value: 'The dataset is benchmarked against CC3M on two most fundamental V+L tasks:
      vision-to-language generation and vision-and-language matching, with an emphasis
      on long-tail visual recognition. The results illustrate the benefit of scaling
      up pre-training data for vision-and-language tasks, as indicated by the new
      state-of-the-art results on both the nocaps and Conceptual Captions benchmarks.

      '
  created_date:
    explanation: 'The date the [[paper]](https://arxiv.org/abs/2102.08981) was submitted.

      '
    value: 2021-02-17
  datasheet: none
  dependencies: []
  description: 'A dataset with 12 million image-text pairs specifically meant to be
    used for vision-and-language pre-training.

    '
  excluded:
    explanation: See [[Relaxing filters for higher recall]](https://arxiv.org/pdf/2102.08981.pdf#subsection.2.2)
    value: 'Some of the filtering steps used in the preparation of Conceptual Captions
      dataset are relaxed to trade off high-recall for low-precision. The following
      steps are applied in the given order:

      1. Image-based Filtering - Only keep JPEG images where both dimensions are greater
      than 400 pixels, and the ratio of larger to smaller dimension is no more than
      2.5. Exclude images that trigger pornography or profanity detectors.

      2. Text-based Filtering - Allow text between 3 and 256 words in the alt-text.
      Discard candidates with no noun or no determiner, but permit ones without prepositions.
      Set the maximum fraction of word repetition allowed to 0.2. Increase the threshold
      for counting a word type as rare from 5 to 20.

      3. Image&Text-based Filtering - Filter out candidates for which none of the
      text tokens can be mapped to the content of the image.

      '
  feedback: Feedback can be provided by creating an issue in the [[Conceptual 12M
    GitHub repository]](https://github.com/google-research-datasets/conceptual-12m)
    or by emailing at conceptual-captions at google.com
  included: ''
  intended_uses: ''
  license:
    explanation: '[[Conceptual Captions License]](https://github.com/google-research-datasets/conceptual-captions/blob/master/LICENSE)

      '
    value: Conceptual Captions License
  modality: image, text
  monitoring: unknown
  name: Conceptual 12M
  nationality: USA
  organization: Google
  prohibited_uses: unknown
  quality_control:
    explanation: See [[Vision-and-Language Pre-Training Data]](https://arxiv.org/pdf/2102.08981.pdf#section.2)
    value: Input candidate (image, caption) pairs pass through several stages of filtering
      and processing to ensure quality. Person-name substitutions are performed in
      the alt-texts to protect the privacy of individuals in the associated images.
  sample: []
  size: 12M (image, text) pairs
  type: dataset
  url: https://arxiv.org/pdf/2102.08981.pdf
- access:
    explanation: https://huggingface.co/t5-large
    value: open
  analysis: https://huggingface.co/t5-base#evaluation
  created_date:
    explanation: The date the T5 paper was made public.
    value: 2019-10-23
  dependencies:
  - C4
  description: Text-To-Text Transfer Transformer (T5) is a model that unifies all
    NLP tasks under the text-to-text format.
  feedback: https://huggingface.co/t5-large/discussions
  intended_uses: NLP tasks
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/t5-base
  monitoring: none
  name: T5
  nationality: USA
  organization: Google
  prohibited_uses: unknown
  quality_control: The T5 paper documents many analyses/ablations that were considered
    before arriving at the final architecture/training procedure.
  size: 11B parameters (dense)
  training_emissions: unknown
  training_hardware: 1,024 TPU v3 chips (Cloud TPU Pods)
  training_time: unknown
  type: model
  url: https://arxiv.org/abs/1910.10683
- access: closed
  analysis: unknown
  created_date:
    explanation: 'The date of the Google product update blog announcing that BERT
      models were for ranking and featured snippets in Search.

      '
    value: 2019-11-25
  dependencies:
  - Internal Google BERT dataset
  description: 'Internal Google BERT model used to power Google Search products.

    '
  feedback: unknown
  intended_uses:
    explanation: 'We don''t have an exhaustive list of the intended use cases for
      the Internal Google BERT model, but we know that Google Search was powered by
      a fine-tuned BERT.

      '
    value: unknown
  license: unknown
  modality: text; text
  model_card: unknown
  monitoring: unknown
  name: Internal Google BERT
  nationality: USA
  organization: Google
  prohibited_uses: unknown
  quality_control: unknown
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://blog.google/products/search/search-language-understanding-bert/
- access: open
  adaptation: unknown
  created_date:
    explanation: 'The date of the Google product update blog announcing that BERT
      models were for ranking and featured snippets in Search.

      '
    value: 2019-11-25
  dependencies:
  - Internal Google BERT
  - MUM
  description: 'Google Search is Google''s search engine.

    '
  failures: unknown
  feedback: 'Feedback can be sent to Google Feedback using the product interface [[Google
    Feedback]](https://www.google.com/tools/feedback).

    '
  intended_uses: Searching the web using text, voice or image
  license: none
  monitoring: 'It is implied that Google scan uses of its products for spam, malware
    and illegal content in the [[Term of Service]](https://policies.google.com/terms).

    '
  monthly_active_users: unknown
  name: Google Search
  nationality: USA
  organization: Google
  output_space: web page ranking
  prohibited_uses: 'Prohibited use cases aren''t specifically spelled out for Google
    search, but several illegal and discouraged use cases are shared in the Respect
    Others section of the [[Term of Service]](https://policies.google.com/terms).

    '
  quality_control: unknown
  terms_of_service: https://policies.google.com/terms
  type: application
  url: https://blog.google/products/search/search-language-understanding-bert/
  user_distribution: unknown
- access: closed
  analysis: unknown
  created_date:
    explanation: 'The date of the Google company news blog announcing LaMDA [[Google
      News Blog]](https://blog.google/technology/ai/lamda/).

      '
    value: 2021-06-18
  datasheet: none
  dependencies: []
  description: 'Infiniset "is a combination of dialog data from public dialog data
    and other public web documents" [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).

    '
  excluded: unknown
  feedback:
    explanation: 'Author contact information was not provided.

      '
    value: none
  included: 'Included in the dataset are data from "public forums (0%); C4 data (12.5%
    ); code documents from sites related to programming like Q&A sites tutorials,
    etc (12.5%); Wikipedia (English) (12.5%); English web documents (6.25%); and Non-English
    web documents (6.25%)."

    '
  intended_uses:
    explanation: 'Intended uses of the dataset wasn''t explicitly linked, but it is
      likely intended for training language models specialized in dialogue.

      '
    value: unknown
  license: unknown
  modality: code, text
  monitoring: unknown
  name: Infiniset
  nationality: USA
  organization: Google
  prohibited_uses: 'The prohibited uses for Infiniset weren''t specifically listed,
    but the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1)
    advises avoiding harm, unjust impact and misinformation, among others.

    '
  quality_control: unknown
  sample: []
  size:
    explanation: 'The size of the dataset is unclear, but it is reported that the
      dataset "consists of 2.97B documents and 1.12B dialogs with 13.39B utterances"
      [[Appendix E]](https://arxiv.org/pdf/2201.08239.pdf#appendix.E).

      '
    value: unknown
  type: dataset
  url: https://arxiv.org/pdf/2201.08239.pdf
- access: closed
  analysis: 'The model performance was analyzed on sensibleness, specificity and interestingness.
    The model was also analyzed on safety, following metrics derived from Google AI
    Principles [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1).
    Finally, the model was analyzed on groundedness, testing its ability to produce
    responses that can be associated with "known sources whenever possible [[Section
    4.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.4.1)."

    '
  created_date:
    explanation: 'The date of the Google company news blog announcing LaMDA [[Google
      News Blog]](https://blog.google/technology/ai/lamda/).

      '
    value: 2021-06-18
  dependencies:
  - Infiniset
  description: 'LaMDA stands for Language Models for Dialog Application. It is a transformer
    based language model trained on dialogue data.

    '
  feedback:
    explanation: 'Author contact information was not provided.

      '
    value: none
  intended_uses: 'LaMDA is a language model, so it can be used for regular langauge
    modelling tasks without fine-tuning, but its fine-tuned for dialogue tasks.

    '
  license: unknown
  modality: text; text
  model_card: none
  monitoring: unknown
  name: LaMDA
  nationality: USA
  organization: Google
  prohibited_uses: 'The prohibited uses of LaMDA weren''t specifically listed, but
    the Google AI principles inspired safety objectives in [[Appendix A.1]](https://arxiv.org/pdf/2201.08239.pdf#subsection.A.1)
    advises avoiding harm, unjust impact and misinformation, among others.

    '
  quality_control: 'LaMDA was fine-tuned to predict sensibleness, specificity and
    interestingness as well as safety. Then, the candidates were filtered out if the
    model safety predictions were below a certain threshold. The next candidates in
    the conversation were selected as a combination of these predictions. The model
    was also fine-tuned for groundedness. The results are shown in [[Figure 5]](https://arxiv.org/pdf/2201.08239.pdf#figure.caption.23).

    '
  size: 137B parameters (dense)
  training_emissions:
    explanation: "\"...total carbon footprint of LaMDA\u2019s pre-training of the\
      \ largest model is approximately 25.2 tCO2e. The carbon footprint of pre-training\
      \ of smaller models and fine-tuning of all models is approximately 0.7 tCO2e\
      \ ... which brings the total footprint of LaMDA to approximately 26 tCO2e [[Section\
      \ 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10)\"\n"
    value: 26 tCO2e
  training_hardware:
    explanation: 'Reported in [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10).

      '
    value: 1024 TPU-V3 chips
  training_time:
    explanation: 'The total number of training flops of LaMDA was reported as 3.55E+23
      (3.55E+8 petaflops) [[Section 10]](https://arxiv.org/pdf/2201.08239.pdf#section.10),
      which is equal to 4108.80 = 3.55E+8 / (60 * 60 * 24) petaflop/s-day.

      '
    value: 4108.80 petaflop/s-day
  type: model
  url: https://arxiv.org/pdf/2201.08239.pdf
- access: closed
  analysis: unknown
  created_date:
    explanation: 'The date of the Google AI blog announcing the details of PaLM [[Google
      AI Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).

      '
    value: 2022-04-04
  datasheet: https://arxiv.org/pdf/2204.02311.pdf#appendix.D
  dependencies:
  - Infiniset
  description: 'PaLM dataset "was created for pre-training language models" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).

    '
  excluded: 'GitHub repositories with copyleft licenses were excluded. Programming
    languageges other than the most common 24 were excluded [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).

    '
  feedback:
    explanation: 'Author contact information is shared in the paper [[Paper]](https://arxiv.org/pdf/2204.02311.pdf).

      '
    value: Contact the authors.
  included: 'The dataset is based on Infiniset. It included multilingual text containing
    text from over 100 languages. The breakdown of the data included is as follows:
    Social media conversations (multilingual) 50, Filtered webpages (multilingual)
    27%, BooksCorpus (English) 13%, GitHub (code) 5%, Wikipedia (multilingual) 4%,
    and News (English) 1%. Code was collected from GitHub repositories with appropriate
    licenses, totalling 96GB of source code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).

    '
  intended_uses:
    explanation: 'As stated in [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).

      '
    value: '"The dataset was created for pre-training language models by a team of
      researchers at Google".

      '
  license: unknown
  modality: code, text
  monitoring: unknown
  name: PaLM dataset
  nationality: USA
  organization: Google
  prohibited_uses: '"... should not be used for any of the unacceptable language model
    use cases, e.g., generation of toxic speech" [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).

    '
  quality_control: 'In order to reduce low quality web pages, the web pages were sampled
    according to a "quality score" classifier. Code files were de-duplicated using
    Levenshtein distance [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3).

    '
  sample: []
  size:
    explanation: 'Dataset size in GB is not reported, but the dataset is reported
      to have 780 billion tokens [[Datasheet]](https://arxiv.org/pdf/2204.02311.pdf#appendix.D).
      The code portion of the dataset is reported to be 5% totaling a 196GB of source
      code [[Section 3]](https://arxiv.org/pdf/2204.02311.pdf#section.3). It is unclear
      whether the reported size is before or after de-duplication. Nonetheless, one
      can estimate the dataset size by multiplying 196GB with 20 = 3.92 TB.

      '
    value: 3.92 TB
  type: dataset
  url: https://arxiv.org/pdf/2204.02311.pdf
- access:
    explanation: Weights can be downloaded from [Github](https://github.com/google-research/t5x/blob/main/docs/models.md)
    value: open
  analysis: Evaluated on a variety of standard language datasets.
  created_date:
    explanation: Date paper was released
    value: 2022-10-20
  dependencies:
  - T5
  - Muffin
  - P3
  - NaturalInstructions-v2
  - Flan CoT
  description: Flan-T5 is a version of the T5 language model fine-tuned on instruction
    data
  feedback: https://huggingface.co/google/flan-t5-xxl/discussions
  intended_uses: unknown
  license:
    explanation: License on the [[Github repository]](https://github.com/google-research/google-research)
    value: Apache 2.0
  modality: text; text
  model_card: https://arxiv.org/pdf/2210.11416.pdf
  monitoring: none
  name: Flan-T5
  nationality: USA
  organization: Google
  prohibited_uses: none
  quality_control: Across different multitask datasets, templates and formatting were
    maintained. For the chain-of-thoughts (CoT) data, specific exemplars were used.
  size: 11B parameters (dense)
  training_emissions: Unknown
  training_hardware: 512 v4 TPU Chips
  training_time: Unknown
  type: model
  url: https://arxiv.org/abs/2210.11416
- access:
    explanation: Model weights available for download in the [[Github repo]](https://github.com/google-research/google-research/tree/master/ul2)
    value: open
  analysis: ''
  created_date:
    explanation: Date model paper was released
    value: 2022-05-10
  dependencies:
  - C4
  description: UL2 is a language model trained with a new pretraining objective
  feedback: ''
  intended_uses: ''
  license:
    explanation: 20B checkpoints only for three different iteration steps
    value: Apache 2.0
  modality: text; text
  model_card: ''
  monitoring: ''
  name: UL2
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 20B parameters (dense)
  training_emissions: ''
  training_hardware: 128 TPUv4
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2205.05131
- access:
    explanation: Google does not provide access to Parti for external researchers.
    value: closed
  analysis: ''
  created_date:
    explanation: Date the model website was made public
    value: 2022-06-22
  dependencies:
  - C4
  - LAION-400M
  - FIT400M
  - JFT-4B
  description: Parti is a text-to-image diffusion model
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; image
  model_card: ''
  monitoring: ''
  name: Parti
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 20B parameters (dense)
  training_emissions: ''
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://parti.research.google/
- access: open
  analysis: ''
  created_date:
    explanation: Date the model website was made public
    value: 2022-05-23
  dependencies:
  - LAION-400M
  - Google internal image-text dataset
  description: Imagen is a text-to-image diffusion model
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; image
  model_card: ''
  monitoring: ''
  name: Imagen
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 14B parameters (dense)
  training_emissions: unknown
  training_hardware: 128 TPU-v4
  training_time: unknown
  type: model
  url: https://imagen.research.google/
- access:
    explanation: Model checkpoints can be downloaded from the [[Github repository]](https://github.com/google-research/google-research/tree/master/vatt)
    value: open
  analysis: ''
  created_date:
    explanation: Date the model paper was made public
    value: 2022-04-22
  dependencies:
  - AudioSet
  - HowTo100M
  description: VATT is a family of models trained on multimodal data
  feedback: ''
  intended_uses: ''
  license:
    explanation: License on the [[Github repository]](https://github.com/google-research/google-research)
    value: Apache 2.0
  modality: text; audio, video
  model_card: ''
  monitoring: ''
  name: VATT
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 155M parameters (dense)
  training_emissions: unknown
  training_hardware: 256 TPU-v3
  training_time: 3 days
  type: model
  url: https://arxiv.org/abs/2104.11178
- access:
    explanation: Made available through the PaLM API as of March 14, 2023.
    value: limited
  analysis: '"PaLM is evaluated on English Natural Language Processing (NLP) tasks,
    tasks from BIG-bench, reasoning tasks, code completion tasks, multilingual generation
    and question answering tasks, translation tasks, and bias and toxicity benchmarks"
    [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).

    '
  created_date:
    explanation: 'The date of the Google AI blog announcing the details of PaLM [[Google
      AI Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).

      '
    value: 2022-04-04
  dependencies:
  - PaLM dataset
  description: 'PaLM stands Pathways Language Model, "dense decoder-only Transformer
    model trained with the Pathways system" [[Google ai Blog]](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html).

    '
  feedback:
    explanation: 'Author contact information is shared in the paper [[Paper]](https://arxiv.org/pdf/2204.02311.pdf).

      '
    value: Contact the authors.
  intended_uses: '"The primary use is research on language models, including: research
    on NLP applications like machine translation and question answering, advancing
    fairness and safety research, and understanding limitations of current LLMs. Within
    Google, PaLM is being used for research on a variety of open- ended text and code
    generation tasks, including reasoning [[Section 6.3]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.3)
    and code synthesis and understanding [[Section 6.4]](https://arxiv.org/pdf/2204.02311.pdf#subsection.6.4)"
    [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E).

    '
  license: unknown
  modality: text; text, code
  model_card: https://arxiv.org/pdf/2204.02311.pdf#appendix.E
  monitoring: unknown
  name: PaLM
  nationality: USA
  organization: Google
  prohibited_uses: 'The model "should not be used for downstream applications without
    further analysis on factors in the proposed downstream application [[Model Card]](https://arxiv.org/pdf/2204.02311.pdf#appendix.E)"

    '
  quality_control: Unknown
  size: 540B parameters (dense)
  training_emissions:
    explanation: 'Reported in [[Appendix B]](https://arxiv.org/pdf/2204.02311.pdf#appendix.A)

      '
    value: 271.43 tCO2
  training_hardware:
    explanation: 'Reported in [[Section 4]](https://arxiv.org/pdf/2204.02311.pdf#section.4).

      '
    value: 6144 TPU v4 chips
  training_time:
    explanation: 'Reported in [[Appendix B]](https://arxiv.org/pdf/2204.02311.pdf#appendix.A)

      '
    value: 29600 petaflop/s-days
  type: model
  url: https://arxiv.org/pdf/2204.02311.pdf
- access: limited
  adaptation: ''
  created_date: 2023-03-14
  dependencies:
  - PaLM
  description: "a new developer offering that makes it easy and safe to experiment\
    \ with Google\u2019s language models."
  failures: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  monitoring: ''
  monthly_active_users: ''
  name: PaLM API
  nationality: USA
  organization: Google
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: ''
  type: application
  url: https://developers.googleblog.com/2023/03/announcing-palm-api-and-makersuite.html
  user_distribution: ''
- access: closed
  analysis: ''
  created_date: 2022-12-26
  dependencies:
  - Flan-PaLM
  - MultiMedQA
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Med-PaLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 540B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2212.13138
- access: closed
  analysis: Evaluated on MultiMedBench tasks and radiologist evaluations of model-generated
    chest X-ray reports
  created_date: 2023-07-26
  dependencies:
  - PaLM-E
  - MultiMedBench
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: image, text, genome sequence; text
  model_card: ''
  monitoring: ''
  name: Med-PaLM Multimodal
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 562B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2307.14334.pdf
- access: closed
  analysis: ''
  created_date: 2022-12-26
  dependencies:
  - MedQA
  - MedMCQA
  - PubMedQA
  - MMLU
  - LiveQA
  - Medication QA
  - HealthSearchQA
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: MultiMedQA
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2212.13138
- access: closed
  analysis: ''
  created_date: 2022-10-20
  dependencies:
  - PaLM
  - Muffin
  - P3
  - NaturalInstructions-v2
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Flan-PaLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 540B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2210.11416
- access: closed
  analysis: ''
  created_date: 2022-10-20
  dependencies:
  - U-PaLM
  - Muffin
  - P3
  - NaturalInstructions-v2
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Flan-U-PaLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 540B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2210.11416
- access: open
  analysis: ''
  created_date: 2021-09-03
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: Apache 2.0
  modality: text
  monitoring: ''
  name: Muffin
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 62 tasks
  type: dataset
  url: https://arxiv.org/abs/2109.01652
- access: closed
  analysis: ''
  created_date: 2022-10-20
  dependencies:
  - PaLM
  - PaLM dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: U-PaLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 540B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2210.11399
- access: closed
  analysis: ''
  created_date: 2022-08-16
  dependencies:
  - PaLM
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown (model weights), Apache 2.0 (SayCan code)
  modality: text; robotics trajectories
  model_card: ''
  monitoring: ''
  name: PaLM-SayCan
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 540B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2204.01691
- access: closed
  analysis: ''
  created_date: 2021-12-13
  dependencies:
  - GLaM Web dataset
  - Wikipedia
  - GLaM Conversations dataset
  - GLaM Forums dataset
  - BooksCorpus
  - GLaM News dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: GLaM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 1.2T parameters (sparse)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2112.06905
- access: closed
  analysis: ''
  created_date: 2021-12-13
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: GLaM Web dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: unknown
  type: dataset
  url: https://arxiv.org/abs/2112.06905
- access: closed
  analysis: ''
  created_date: 2021-12-13
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: GLaM Conversations dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: unknown
  type: dataset
  url: https://arxiv.org/abs/2112.06905
- access: closed
  analysis: ''
  created_date: 2021-12-13
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: GLaM Forums dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: unknown
  type: dataset
  url: https://arxiv.org/abs/2112.06905
- access: closed
  analysis: ''
  created_date: 2021-12-13
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: GLaM News dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: unknown
  type: dataset
  url: https://arxiv.org/abs/2112.06905
- access: closed
  analysis: ''
  created_date: 2021-05-18
  dependencies:
  - MUM dataset
  description: MUM (Multitask Unified Model) is a multimodal model that is specialized
    for more complex queries.
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: image, text; text
  model_card: ''
  monitoring: ''
  name: MUM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://blog.google/products/search/introducing-mum/
- access: closed
  analysis: ''
  created_date: 2021-05-18
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: image, text
  monitoring: ''
  name: MUM dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: unknown
  type: dataset
  url: https://blog.google/products/search/introducing-mum/
- access: closed
  analysis: ''
  created_date: 2023-02-01
  dependencies:
  - LAION-400M
  - Phenaki Video-Text Corpus
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; video
  model_card: ''
  monitoring: ''
  name: Phenaki
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 1.8B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://openreview.net/pdf?id=vOEXS39nOF
- access: closed
  analysis: ''
  created_date: 2023-02-01
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text, video
  monitoring: ''
  name: Phenaki Video-Text Corpus
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 15M text-video pairs at 8FPS
  type: dataset
  url: https://openreview.net/pdf?id=vOEXS39nOF
- access: open
  analysis: ''
  created_date: 2023-03-02
  dependencies:
  - UL2
  - Flan Collection
  description: ''
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Flan-UL2
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 20B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2205.05131
- access: open
  analysis: ''
  created_date: 2023-01-31
  datasheet: ''
  dependencies:
  - Flan dataset
  - P3
  - NaturalInstructions-v2
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: Apache 2.0
  modality: text
  monitoring: ''
  name: Flan Collection
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 1836 tasks
  type: dataset
  url: https://arxiv.org/abs/2301.13688
- access: closed
  analysis: ''
  created_date: 2023-01-26
  dependencies:
  - SoundStream
  - w2v-BERT
  - MuLan
  - MusicLM semantic model
  - MusicLM acoustic model
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; audio
  model_card: ''
  monitoring: ''
  name: MusicLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 1.4B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2301.11325.pdf
- access: closed
  analysis: ''
  created_date: 2023-01-26
  dependencies:
  - Free Music Archive
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio; audio
  model_card: ''
  monitoring: ''
  name: SoundStream
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2301.11325.pdf
- access: closed
  analysis: ''
  created_date: 2023-01-26
  dependencies:
  - Free Music Archive
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio; audio
  model_card: ''
  monitoring: ''
  name: w2v-BERT
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 600M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2301.11325.pdf
- access: closed
  analysis: ''
  created_date: 2022-08-26
  dependencies:
  - AST
  - BERT
  - MuLan dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; audio
  model_card: ''
  monitoring: ''
  name: MuLan
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2208.12415
- access: closed
  analysis: ''
  created_date: 2022-08-26
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: audio, text
  monitoring: ''
  name: MuLan dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 370K hours audio
  type: dataset
  url: https://arxiv.org/abs/2208.12415
- access: closed
  analysis: ''
  created_date: 2023-01-26
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: audio
  monitoring: ''
  name: MusicLM dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 280K hours audio
  type: dataset
  url: https://arxiv.org/pdf/2301.11325.pdf
- access: closed
  analysis: ''
  created_date: 2023-01-26
  dependencies:
  - MusicLM dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio; audio
  model_card: ''
  monitoring: ''
  name: MusicLM semantic model
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 430M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2301.11325.pdf
- access: closed
  analysis: ''
  created_date: 2023-01-26
  dependencies:
  - MusicLM dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio; audio
  model_card: ''
  monitoring: ''
  name: MusicLM acoustic model
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 430M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2301.11325.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  dependencies:
  - Noise2Music pseudolabel dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknkown
  modality: audio, text; audio
  model_card: ''
  monitoring: ''
  name: Noise2Music
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  datasheet: ''
  dependencies:
  - LaMDA
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: LaMDA-LF
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 150k songs
  type: dataset
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  datasheet: ''
  dependencies:
  - MusicCaps
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: Rater-LF
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 10k captions
  type: dataset
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  datasheet: ''
  dependencies:
  - MusicCaps
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: Rater-SF
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 24k captions
  type: dataset
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  dependencies:
  - MuLan
  - MuLaMCap
  - LaMDA-LF
  - Rater-LF
  - Rater-SF
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio, text; audio
  model_card: ''
  monitoring: ''
  name: Noise2Music pseudolabeler
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: audio
  monitoring: ''
  name: Noise2Music audio dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 340k hours audio
  type: dataset
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: closed
  analysis: ''
  created_date: 2023-02-08
  datasheet: ''
  dependencies:
  - Noise2Music audio dataset
  - Noise2Music pseudolabeler
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license:
    explanation: 'The asset isn''t released, and hence the license is unknown.

      '''
    value: unknown
  modality: audio, text
  monitoring: ''
  name: Noise2Music pseudolabel dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 340k hours audio with pseudolabels
  type: dataset
  url: https://google-research.github.io/noise2music/noise2music.pdf
- access: limited
  adaptation: ''
  created_date: 2022-08-25
  dependencies:
  - LaMDA
  description: AI Test Kitchen provides a new way for people to learn about, experience,
    and give feedback on emerging AI technology, like LaMDA.
  failures: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  monitoring: ''
  monthly_active_users: ''
  name: AI Test Kitchen
  nationality: USA
  organization: Google
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: ''
  type: application
  url: https://blog.google/technology/ai/join-us-in-the-ai-test-kitchen/
  user_distribution: ''
- access: closed
  adaptation: ''
  created_date: 2023-02-06
  dependencies:
  - LaMDA
  description: Conversational AI service, powered by LaMDA
  failures: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  monitoring: ''
  monthly_active_users: ''
  name: Bard
  nationality: USA
  organization: Google
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: ''
  type: application
  url: https://blog.google/technology/ai/bard-google-ai-search-updates/
  user_distribution: ''
- access: closed
  analysis: ''
  created_date: 2022-06-29
  dependencies:
  - PaLM
  - arXiv
  - PaLM dataset
  - Minerva Math Web Pages dataset
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Minerva
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 540B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2206.14858
- access: closed
  analysis: ''
  created_date: 2022-06-29
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: Minerva Math Web Pages dataset
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 17.5B tokens
  type: dataset
  url: https://arxiv.org/abs/2206.14858
- access: limited
  analysis: ''
  created_date: 2023-03-06
  dependencies:
  - YT-NLU-U
  - Pub-U
  - Web-NTL
  - YT-SUP+
  - Pub-S
  description: Universal Speech Model (USM) is a family of state-of-the-art speech
    models with 2B parameters trained on 12 million hours of speech and 28 billion
    sentences of text, spanning 300+ languages. USM, which is for use in YouTube (e.g.,
    for closed captions), can perform automatic speech recognition (ASR) on widely-spoken
    languages like English and Mandarin, but also languages like Punjabi, Assamese,
    Santhali, Balinese, Shona, Malagasy, Luganda, Luo, Bambara, Soga, Maninka, Xhosa,
    Akan, Lingala, Chichewa, Nkore, Nzema to name a few. Some of these languages are
    spoken by fewer than twenty million people, making it very hard to find the necessary
    training data.
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio, text; text
  model_card: ''
  monitoring: ''
  name: USM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 2B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2303.01037
- access: open
  adaptation: ''
  created_date: 2005-02-14
  dependencies:
  - USM
  description: YouTube is a global online video sharing and social media platform
  failures: ''
  feedback: ''
  intended_uses: ''
  license: ''
  monitoring: ''
  monthly_active_users: ''
  name: YouTube
  nationality: USA
  organization: Google
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: ''
  type: application
  url: https://www.youtube.com/
  user_distribution: ''
- access: closed
  analysis: ''
  created_date: 2023-03-06
  dependencies:
  - PaLM
  - ViT-22B
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: image, text; text
  model_card: ''
  monitoring: ''
  name: PaLM-E
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 562B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2303.03378
- access: closed
  analysis: ''
  created_date: 2023-02-10
  dependencies:
  - JFT
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: image, image
  model_card: ''
  monitoring: ''
  name: ViT-22B
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 22B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2302.05442
- access: closed
  analysis: ''
  created_date: 2022-09-07
  dependencies:
  - w2v-BERT
  - SoundStream
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio, text; audio
  model_card: ''
  monitoring: ''
  name: AudioLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 1B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.03143
- access: closed
  analysis: ''
  created_date: 2022-09-14
  dependencies:
  - mT5
  - ViT-e
  - WebLI
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; image
  model_card: ''
  monitoring: ''
  name: PaLI
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 17B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.06794
- access: closed
  analysis: ''
  created_date: 2022-09-14
  dependencies:
  - JFT
  description: ''
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: image; image
  model_card: ''
  monitoring: ''
  name: ViT-e
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 3.9B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2209.06794
- access: closed
  analysis: ''
  created_date: 2022-09-14
  datasheet: ''
  dependencies: []
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: unknown
  modality: image, text
  monitoring: ''
  name: WebLI
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 10B images, 12B alt-text
  type: dataset
  url: https://arxiv.org/abs/2209.06794
- access: open
  analysis: ''
  created_date: 2023-02-27
  dependencies:
  - T5
  - CLIP
  - YT-Temporal-1B
  description: ''
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality: video; text
  model_card: ''
  monitoring: ''
  name: Vid2Seq
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 500M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2302.14115
- access: closed
  analysis: evaluated on DSTC11 Challenge Task, based on MultiWoz 2.1, with a focus
    on dialog state tracking.
  created_date: 2023-06-08
  dependencies:
  - CTC blank-filtering
  - Speech2Text adapter
  description: Joint speech and language model using a Speech2Text adapter and using
    a CTC-based blank-filtering.
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: audio; text
  model_card: ''
  monitoring: ''
  name: Google Joint SLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2306.07944.pdf
- access: closed
  analysis: Reports results on standard code benchmarks across a variety of programming
    languages.
  created_date: 2023-05-10
  dependencies:
  - PaLM 2 dataset
  description: PaLM 2 is a new state-of-the-art language model that has better multilingual
    and reasoning capabilities and is more compute-efficient than its predecessor
    PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives
    similar to UL2.
  feedback: Specific queries provided by annotators
  intended_uses: general use large language model that can be used for language, reasoning,
    and code tasks.
  license: unknown
  modality: text; text
  model_card: https://ai.google/static/documents/palm2techreport.pdf
  monitoring: Google internal monitoring
  name: PaLM 2
  nationality: USA
  organization: Google
  prohibited_uses: becoming part of a general-purpose service or product or use within
    specific downstream applications without prior assessment
  quality_control: Employed de-duplication, removal of sensitive-PII and filtering.
    Added control tokens marking toxicity of text.
  size: unknown
  training_emissions: ''
  training_hardware: TPU v4 (number unspecified)
  training_time: ''
  type: model
  url: https://blog.google/technology/ai/google-palm-2-ai-large-language-model/
- access: limited
  analysis: Assessed on medical benchmarks of professional medical exams, medical
    research, and consumer queries.
  created_date: 2023-12-13
  dependencies: []
  description: MedLM is a collection of foundation models tuned to follow natural
    language instructions for tasks in medicine, such as question answering and creating
    draft summaries.
  feedback: none
  intended_uses: to be used for question answering and creating draft summaries from
    existing documentation, to be reviewed, edited, and approved by the user before
    use.
  license: unknown
  modality: text; text
  model_card: https://cloud.google.com/static/vertex-ai/docs/generative-ai/medlm/MedLM-model-card.pdf
  monitoring: Google internal monitoring
  name: MedLM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://cloud.google.com/vertex-ai/docs/generative-ai/medlm/overview
- access: closed
  analysis: Evaluated on standard general, reasoning, math, coding, and multimodal
    benchmarks with results that surpass GPT-4 on almost all.
  created_date: 2023-12-06
  dependencies: []
  description: As of release, Gemini is Google's most capable and flexible AI model,
    proficient in multimodal domains.
  feedback: none
  intended_uses: general use large language model that can be used for language, reasoning,
    and code tasks.
  license: unknown
  modality: text; image, text, video
  model_card: none
  monitoring: Google internal monitoring
  name: Gemini
  nationality: USA
  organization: Google
  prohibited_uses: becoming part of a general-purpose service or product or use within
    specific downstream applications without prior assessment
  quality_control: ''
  size:
    explanation: Comes in sizes Ultra, Pro, and Nano.
    value: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://deepmind.google/technologies/gemini/#introduction
- access: closed
  analysis: Evaluated on popular time-series benchmarks.
  created_date: 2024-02-02
  dependencies: []
  description: TimesFM is a single forecasting model pre-trained on a large time-series
    corpus of 100 billion real world time-points.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: ''
  model_card: none
  monitoring: unknown
  name: TimesFM
  nationality: USA
  organization: Google
  prohibited_uses: ''
  quality_control: ''
  size: 200M parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://blog.research.google/2024/02/a-decoder-only-foundation-model-for.html
- access: open
  analysis: Evaluation was conducted on standard LLM benchmarks and includes internal
    red-teaming testing of relevant content policies.
  created_date: 2024-02-21
  dependencies: []
  description: Gemma is a family of lightweight, state-of-the-art open models from
    Google, based on the Gemini models. They are text-to-text, decoder-only large
    language models, available in English.
  feedback: https://huggingface.co/google/gemma-7b/discussions
  intended_uses: Text generation tasks including question answering, summarization,
    and reasoning; content creation, communication, research, and education.
  license:
    explanation: License can be found at https://ai.google.dev/gemma/terms.
    value: custom
  modality: text; text
  model_card: https://huggingface.co/google/gemma-7b
  monitoring: ''
  name: Gemma
  nationality: USA
  organization: Google
  prohibited_uses: Prohibited uses are specified in the Gemma Prohibited Use Policy
    here https://ai.google.dev/gemma/prohibited_use_policy
  quality_control: Multiple evaluations and red-teaming conducted, with particular
    focus on ethics, bias, fair use cases, and safety.
  size: 7B parameters (dense)
  training_emissions: unknown
  training_hardware: TPUv5e
  training_time: unknown
  type: model
  url: https://blog.google/technology/developers/gemma-open-models/
- access: closed
  analysis: Evaluated Med-Gemini on 14 medical benchmarks spanning text, multimodal
    and long-context applications, establishing new state-of-the-art (SoTA) performance
    on 10 of them, and surpassing the GPT-4 model family on every benchmark where
    a direct comparison is viable.
  created_date: 2024-04-29
  dependencies:
  - Gemini
  - MultiMedBench
  description: Med-Gemini is a family of highly capable multimodal models that are
    specialized in medicine with the ability to seamlessly integrate the use of web
    search, and that can be efficiently tailored to novel modalities using custom
    encoders.
  feedback: none
  intended_uses: To be used in areas of medical research including medical summarization,
    referral letter generation, and medical simplification tasks.
  license: unknown
  modality: image, text; text
  model_card: none
  monitoring: ''
  name: Med-Gemini
  nationality: USA
  organization: Google
  prohibited_uses: Unfit for real-world deployment in the safety-critical medical
    domain.
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2404.18416
- access: open
  analysis: Unknown
  created_date: 2024-09-05
  dependencies: []
  description: Imagen 3 is a high-quality text-to-image model capable of generating
    images with improved detail, richer lighting, and fewer distracting artifacts.
    It features improved prompt understanding and can be used to generate a wide array
    of visual styles from quick sketches to high-resolution images. The model is available
    in multiple versions, each optimized for particular types of tasks. Imagen 3 has
    been trained to capture nuances like specific camera angles or compositions in
    long, complex prompts, making it a versatile tool for image generation from textual
    inputs.
  feedback: Unknown
  intended_uses: Imagen 3 is intended to be used for generation of high-resolution
    images from textual prompts, from photorealistic landscapes to richly textured
    oil paintings or whimsical claymation scenes. It can also be used for stylized
    birthday cards, presentations, and more, due to its improved text rendering capabilities.
  license: Unknown
  modality: text; image
  model_card: unknown
  monitoring: Unknown
  name: Imagen 3
  nationality: unknown
  organization: Google DeepMind
  prohibited_uses: Unknown
  quality_control: Unknown
  size: Unknown
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://deepmind.google/technologies/imagen-3/
- access: open
  analysis: The 27B Gemma 2 model outperforms other open models in its size category
    offering cutting-edge performance. Specific details can be found in the provided
    technical report.
  created_date: 2024-06-27
  dependencies:
  - Gemma
  - CodeGemma
  - RecurrentGemma
  - PaliGemma
  description: Gemma 2 is an open model that offers best-in-class performance and
    runs at incredible speed across different hardware. It easily integrates with
    other AI tools. This model is built on a redesigned architecture engineered for
    exceptional performance and inference efficiency. It is available in both 9 billion
    (9B) and 27 billion (27B) parameter sizes. Gemma 2 is optimized to run at incredible
    speed across a range of hardware, from powerful gaming laptops and high-end desktops,
    to cloud-based setups.
  feedback: Unknown
  intended_uses: Gemma 2 is designed for developers and researchers for various AI
    tasks. It can be used via the integrations it offers with other AI tools/platforms
    and can additionally be deployed for more accessible and budget-friendly AI deployments.
  license: Gemma (commercially-friendly license given by Google DeepMind)
  modality: text; text
  model_card: unknown
  monitoring: Unknown
  name: Gemma 2
  nationality: unknown
  organization: Google DeepMind
  prohibited_uses: Not specified
  quality_control: Google DeepMind implemented a refined architecture for Gemma 2.
    The model has improvements in safety and efficiency over the first generation.
    The deployment of Gemma 2 on Vertex AI, scheduled for the next month, will offer
    effortless management of the model.
  size: 27B parameters (dense)
  training_emissions: Unknown
  training_hardware: Google Cloud TPU host, NVIDIA A100 80GB Tensor Core GPU, NVIDIA
    H100 Tensor Core GPU
  training_time: Unknown
  type: model
  url: https://blog.google/technology/developers/google-gemma-2/
- access: open
  analysis: The model nearly matches the performance of the Gemini 1.5 Flash model
    across many benchmarks through developer feedback and testing.
  created_date: 2024-10-03
  dependencies: []
  description: Gemini 1.5 Flash-8B is a lightweight, speed-optimized variant of the
    Gemini 1.5 Flash model, designed for efficiency and lower latency especially for
    tasks such as chat, transcription, and long context language translation. The
    model is production-ready and offers the lowest cost per intelligence of any Gemini
    model with improved rate limits.
  feedback: Encourages developer feedback to inform model improvements and future
    updates.
  intended_uses: Designed for simple, high-volume tasks including multimodal use cases
    and long-context summarization.
  license: unknown
  modality: audio, image, text, video; text
  model_card: unknown
  monitoring: unknown
  name: Gemini 1.5 Flash-8B
  nationality: unknown
  organization: Google DeepMind
  prohibited_uses: unknown
  quality_control: Model improvement informed by developer feedback and extensive
    testing.
  size: 8B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://developers.googleblog.com/en/gemini-15-flash-8b-is-now-generally-available-for-use/
- access:
    explanation: The models are accessible via Vertex AI Model Garden [CXR, Derm,
      Path] and Hugging Face [CXR, Derm, Path].
    value: open
  analysis: After giving the community time to use the models and explore different
    applications, we collected feedback.
  created_date: 2024-11-25
  dependencies:
  - EfficientNet-L2
  - BERT
  - CLIP
  - BLIP-2
  - BiT ResNet-101x3
  - ViT-S
  description: Health AI Developer Foundations (HAI-DEF) is a new suite of open weight
    models to help developers more easily build AI models for healthcare applications.
  feedback: unknown
  intended_uses: Building AI models for healthcare applications including imaging
    applications in radiology, dermatology, and pathology.
  license: unknown
  modality:
    explanation: "CXR Foundation\u2019s image encoding model takes DICOM images, and\
      \ its text-encoder accepts textual strings..."
    value: text, image; vector
  model_card: unknown
  monitoring: After giving the community time to use the models and explore different
    applications, we collected feedback.
  name: Health AI Developer Foundations (HAI-DEF)
  nationality: USA
  organization: Google Research
  prohibited_uses: unknown
  quality_control: Learning from community experience with previous research endpoints...we
    collected feedback.
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: http://goo.gle/3AWVX33
- access: closed
  analysis: Unknown
  created_date: 2024-12-04
  dependencies:
  - Imagen 3
  description: Genie 2 is a foundation world model capable of generating an endless
    variety of action-controllable, playable 3D environments for training and evaluating
    embodied agents based on a single prompt image.
  feedback: Unknown
  intended_uses: Genie 2 can be used for generating diverse environments for training
    and evaluating AI agents, rapid prototyping interactive experiences, and experimenting
    with novel environments.
  license: Unknown
  modality:
    explanation: the model is prompted with a single image generated by Imagen 3
    value: image; video
  model_card: unknown
  monitoring: Unknown
  name: Genie 2
  nationality: unknown
  organization: Google DeepMind
  prohibited_uses: Unknown
  quality_control: Responsible development is emphasized, developing our technologies
    responsibly and building towards more general AI systems that can safely carry
    out tasks.
  size: Unknown
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/
- access: limited
  analysis: Veo 2 outperforms other leading video generation models, based on human
    evaluations of its performance.
  created_date: 2024-12-16
  dependencies: []
  description: Veo 2 is a state-of-the-art video generation model that creates videos
    with realistic motion and high-quality output, up to 4K, with extensive camera
    controls. It simulates real-world physics and offers advanced motion capabilities
    with enhanced realism and fidelity.
  feedback: unknown
  intended_uses: Creating high-quality videos with realistic motion, different styles,
    camera controls, shot styles, angles, and movements.
  license: unknown
  modality:
    explanation: Our state-of-the-art video generation model ... text-to-image model
      Veo 2
    value: text; video
  model_card: unknown
  monitoring: unknown
  name: Veo 2
  nationality: unknown
  organization: Google DeepMind
  prohibited_uses: unknown
  quality_control: Veo 2 includes features that enhance realism, fidelity, detail,
    and artifact reduction to ensure high-quality output.
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://deepmind.google/technologies/veo/veo-2/
- access:
    explanation: Gemini 2.0 Flash is available to developers and trusted testers,
      with wider availability planned for early next year.
    value: limited
  analysis: unknown
  created_date: 2024-12-11
  dependencies: []
  description: Google DeepMind introduces Gemini 2.0, a new AI model designed for
    the 'agentic era.'
  feedback: unknown
  intended_uses: Develop more agentic models, meaning they can understand more about
    the world around you, think multiple steps ahead, and take action on your behalf,
    with your supervision.
  license: unknown
  modality:
    explanation: The first model built to be natively multimodal, Gemini 1.0 and 1.5
      drove big advances with multimodality and long context to understand information
      across text, video, images, audio and code...
    value: text, video, image, audio; image, text
  model_card: unknown
  monitoring: unknown
  name: Gemini 2.0
  nationality: unknown
  organization: Google DeepMind
  prohibited_uses: unknown
  quality_control: Google is committed to building AI responsibly, with safety and
    security as key priorities.
  size: unknown
  training_emissions: unknown
  training_hardware:
    explanation: "It\u2019s built on custom hardware like Trillium, our sixth-generation\
      \ TPUs."
    value: custom hardware like Trillium, our sixth-generation TPUs
  training_time: unknown
  type: model
  url: https://blog.google/technology/google-deepmind/google-gemini-ai-update-december-2024/#ceo-message
- access:
    explanation: 'Download models & code: Find the pre-trained models and code on
      Hugging Face and Kaggle'
    value: open
  analysis: Our research demonstrates leading performance on chemical formula recognition,
    music score recognition, spatial reasoning, and chest X-ray report generation,
    as detailed in the technical report.
  created_date: 2024-12-05
  dependencies:
  - PaliGemma
  - Hugging Face
  - Kaggle
  - Hugging Face Transformers
  - Keras
  - PyTorch
  - JAX
  description: PaliGemma 2 builds upon the performant Gemma 2 models, adding the power
    of vision and making it easier than ever to fine-tune for exceptional performance.
    With PaliGemma 2, these models can see, understand, and interact with visual input,
    opening up a world of new possibilities.
  feedback: Your feedback and contributions are invaluable in shaping the future of
    these models and driving innovation in the field.
  intended_uses: fine-tuning for specific tasks and datasets straightforward, empowering
    you to tailor its capabilities to your precise needs.
  license: unknown
  modality:
    explanation: PaliGemma 2 generates detailed, contextually relevant captions for
      images
    value: image; text
  model_card: unknown
  monitoring: unknown
  name: PaliGemma 2
  nationality: USA
  organization: Google
  prohibited_uses: unknown
  quality_control: unknown
  size:
    explanation: Optimize performance for any task with PaliGemma 2's multiple model
      sizes (3B, 10B, 28B parameters)
    value: 28B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://developers.googleblog.com/en/introducing-paligemma-2-powerful-vision-language-models-simple-fine-tuning/
