- access: open
  analysis: ''
  created_date: 2022-12-06
  dependencies:
  - Kinetics-400
  - WebVid-2M
  - WebVid-10M
  - HowTo100M
  - AVA
  - Something-Something-v2
  - Kinetics-710
  description: ''
  feedback: ''
  intended_uses: ''
  license: Apache 2.0
  modality: text, video; video
  model_card: ''
  monitoring: ''
  name: InternVideo
  nationality: China
  organization: Shanghai AI Laboratory
  prohibited_uses: ''
  quality_control: ''
  size: 1.3B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2212.03191.pdf
- access: open
  analysis: Evaluated based on own constructed dataset covering 433 languages.
  created_date: 2023-05-29
  dependencies:
  - OPUS
  description: Lego-MT is a multilingual large language model which uses a more efficient
    approach of being an effective detachable model.
  feedback: ''
  intended_uses: ''
  license: ''
  modality: text; text
  model_card: ''
  monitoring: ''
  name: Lego-MT
  nationality: China
  organization: Shanghai AI Laboratory
  prohibited_uses: ''
  quality_control: ''
  size: 1.2B parameters (dense)
  training_emissions: unknown
  training_hardware: 32 A100 GPUs
  training_time: 15 days
  type: model
  url: https://arxiv.org/pdf/2212.10551.pdf
- access: open
  analysis: Evaluated on GSM8K and the competition-level MATH dataset.
  created_date: 2023-10-05
  dependencies:
  - GPT-4
  - LLaMA 2
  description: MathCoder is a family of models capable of generating code-based solutions
    for solving challenging math problems.
  feedback: none
  intended_uses: bridging the gap between natural language understanding and computational
    problem-solving
  license: unknown
  modality: text; text
  model_card: none
  monitoring: none
  name: MathCoder
  nationality: China
  organization: Shanghai AI Laboratory
  prohibited_uses: none
  quality_control: none
  size: 70B parameters (dense)
  training_emissions: unknown
  training_hardware: 32 NVIDIA A800 80GB GPUs
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2310.03731.pdf
- access: open
  analysis: Evaluated in comparison to LLaMA series models on standard benchmarks.
  created_date: 2023-09-20
  dependencies: []
  description: InternLM is a high-quality language model proficient in English, Chinese,
    and code.
  feedback: https://huggingface.co/internlm/internlm-20b/discussions
  intended_uses: ''
  license: Apache 2.0
  modality: code, text; code, text
  model_card: https://huggingface.co/internlm/internlm-20b
  monitoring: none
  name: InternLM
  nationality: China
  organization: Shanghai AI Laboratory
  prohibited_uses: ''
  quality_control: ''
  size: 20B parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://github.com/InternLM/InternLM
- access: open
  analysis: Evaluated across a range of video-related tasks and compared to other
    multimodal models like CLIP, VideoPrism, and VideoCoCa. InternVideo 2 generally
    performs among the best of such models on these benchmarks.
  created_date: 2024-03-22
  dependencies:
  - InternVL
  - VideoMAEv2
  - LAION
  - WebVid
  - InternVid
  - LLaVA
  - KMash
  description: InternVideo2 is a new video foundation model (ViFM) that achieves the
    state-of-the-art performance in action recognition, video-text tasks, and video-centric
    dialogue.
  feedback: none
  intended_uses: ''
  license: MIT
  modality: text, video; text
  model_card: none
  monitoring: unknown
  name: InternVideo2
  nationality: China
  organization: Shanghai AI Laboratory, Nanjing University, Zhejiang University
  prohibited_uses: ''
  quality_control: ''
  size: 6B parameters
  training_emissions: unknown
  training_hardware: 256 NVIDIA A100 GPUs for 32 days, and 64 GPUs for 3 days
  training_time: 35 days
  type: model
  url: https://github.com/OpenGVLab/InternVideo2
- access: open
  analysis: The model was compared with SOTAs and has shown good performance in generating
    high-quality human images.
  created_date: 2024-04-01
  dependencies:
  - CosmicMan-HQ 1.0
  description: CosmicMan is a text-to-image foundation model specialized for generating
    high-fidelity human images with meticulous appearance, reasonable structure, and
    precise text-image alignment.
  feedback: unknown
  intended_uses: The model is intended to generate high-quality, photorealistic human
    images from text descriptions. Applications include avatar generation and potentially
    virtual reality and video game character creation.
  license: unknown
  modality: text; image
  model_card: none
  monitoring: unknown
  name: CosmicMan
  nationality: China
  organization: Shanghai AI Laboratory
  prohibited_uses: unknown
  quality_control: The quality control measures taken include modeling the relationship
    between dense text descriptions and image pixels in a decomposed manner and enforcing
    attention refocusing without adding extra modules.
  size: unknown
  training_emissions: unknown
  training_hardware: 32 80G NVIDIA A100 GPUs
  training_time: 1 week
  type: model
  url: https://cosmicman-cvpr2024.github.io/
- access: open
  analysis: Compared to other human image datasets on data quantity, image quality,
    and annotations.
  created_date: 2024-04-28
  datasheet: none
  dependencies: []
  description: CosmicMan-HQ 1.0 is a large-scale dataset with 6 million high-quality,
    real-world human images.
  excluded: ''
  feedback: none
  included: ''
  intended_uses: ''
  license: unknown
  modality: image
  monitoring: unknown
  name: CosmicMan-HQ 1.0
  nationality: China
  organization: Shanghai AI Laboratory
  prohibited_uses: ''
  quality_control: unknown
  sample: []
  size: 6 million images
  type: dataset
  url: https://arxiv.org/pdf/2404.01294
