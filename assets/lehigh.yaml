- access: open
  analysis: outperforms majority of preceding state-of-the-art models over 15 unique
    biomedical modalities.
  created_date: 2023-05-26
  dependencies:
  - GPT-style autoregressive decoder
  - BiomedGPT biomedical datasets
  description: BiomedGPT leverages self-supervision on large and diverse datasets
    to accept multi-modal inputs and perform a range of downstream tasks.
  feedback: ''
  intended_uses: furthering research in developing unified and generalist models for
    biomedicine.
  license: Apache 2.0
  modality: image, text; text
  model_card: ''
  monitoring: ''
  name: BiomedGPT
  nationality: USA
  organization: Lehigh University
  prohibited_uses: ''
  quality_control: No specific quality control is mentioned in model training, though
    details on data processing and how the model was trained are provided in the paper.
  size: 472M parameters (dense)
  training_emissions: unknown
  training_hardware: 10 NVIDIA A5000 GPUs
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2305.17100.pdf
