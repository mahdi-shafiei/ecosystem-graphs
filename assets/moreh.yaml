- access: open
  analysis: unknown
  created_date: 2024-01-16
  dependencies:
  - Qwen
  - OpenOrca
  description: MoMo is a large language model fine-tuned from Qwen.
  feedback: https://huggingface.co/moreh/MoMo-72B-lora-1.8.7-DPO/discussions
  intended_uses: ''
  license: MIT
  modality: text; text
  model_card: https://huggingface.co/moreh/MoMo-72B-lora-1.8.7-DPO
  monitoring: unknown
  name: MoMo
  nationality: USA
  organization: Moreh
  prohibited_uses: ''
  quality_control: unknown
  size: 72B parameters (dense)
  training_emissions: unknown
  training_hardware: "AMD\u2019s MI250 GPU"
  training_time: unknown
  type: model
  url: https://huggingface.co/moreh/MoMo-72B-lora-1.8.7-DPO
