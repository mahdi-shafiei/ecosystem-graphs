- access: open
  analysis: Evaluated on public benchmarks like MMLU, CMMLU, GSM8K, and HumanEval.
  created_date: 2023-09-20
  dependencies: []
  description: Baichuan 2 is a series of large-scale multilingual language models
    containing 7 billion and 13 billion parameters, trained from scratch, on 2.6 trillion
    tokens.
  feedback: https://huggingface.co/Xwin-LM/Xwin-LM-70B-V0.1/discussions
  intended_uses: ''
  license: Apache 2.0
  modality: text; text
  model_card: none
  monitoring: none
  name: Baichuan 2
  nationality: China
  organization: Baichuan Inc.
  prohibited_uses: ''
  quality_control: ''
  size: 13B parameters (dense)
  training_emissions: unknown
  training_hardware: 1024 NVIDIA A800 GPUs
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2309.10305.pdf
