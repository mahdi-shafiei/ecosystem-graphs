- access: open
  analysis: Tested on several benchmarks, most notably Python benchmark HumanEval.
  created_date: 2023-05-09
  dependencies:
  - The Stack
  description: StarCoder is a Large Language Model for Code (Code LLM) trained on
    permissively licensed data from GitHub, including from 80+ programming languages,
    Git commits, GitHub issues, and Jupyter notebooks.
  feedback: https://huggingface.co/bigcode/starcoder/discussions
  intended_uses: As a foundation model to fine-tune and create more specialized models
    that support use cases such as code completion, fill-in-the-middle, and text summarization.
    Can also be used as a Tech Assistant prompt and not as an instruction model given
    training limitations.
  license: BigCode Open RAIL-M v1.0
  modality: code; code
  model_card: https://huggingface.co/bigcode/starcoder
  monitoring: ''
  name: StarCoder
  nationality: International
  organization: BigCode
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  quality_control: No specific quality control is mentioned in model training, though
    details on data processing and how the tokenizer was trained are provided in the
    paper.
  size: 15.5B parameters (dense)
  training_emissions: 16.68 tons of CO2eq
  training_hardware: 512 A100 80GB GPUs distributed across 64 nodes
  training_time: 320,256 GPU hours
  type: model
  url: https://arxiv.org/pdf/2305.06161.pdf
- access: open
  analysis: Evaluated on MultiPL-E system benchmarks.
  created_date: 2023-02-24
  dependencies:
  - The Stack
  - BigCode Dataset
  description: Multilingual code model derived from the findings of BigCode Project
    analysis on Github stars' association to data quality.
  feedback: https://huggingface.co/bigcode/santacoder/discussions
  intended_uses: The model was trained on GitHub code. As such it is not an instruction
    model and commands do not work well. You should phrase commands like they occur
    in source code such as comments or write a function signature and docstring and
    let the model complete the function body.
  license: BigCode Open RAIL-M v1
  modality: code; code
  model_card: https://huggingface.co/bigcode/santacoder
  monitoring: ''
  name: SantaCoder
  nationality: International
  organization: BigCode
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  quality_control: ''
  size: 1.1B parameters (dense)
  training_emissions: 124 kg of CO2eq
  training_hardware: 96 NVIDIA Tesla V100 GPUs
  training_time: 14,284 GPU hours
  type: model
  url: https://arxiv.org/pdf/2301.03988.pdf
- access: open
  analysis: Evaluated models trained on The Stack on HumanEval and MBPP and compared
    against similarly-sized models.
  created_date: 2022-11-20
  datasheet: https://huggingface.co/datasets/bigcode/the-stack
  dependencies:
  - GitHub
  description: The Stack contains over 6TB of permissively-licensed source code files
    covering 358 programming languages. The Stack serves as a pre-training dataset
    for Code LLMs, i.e., code-generating AI systems which enable the synthesis of
    programs from natural language descriptions as well as other from code snippets.
  excluded: ''
  feedback: https://huggingface.co/datasets/bigcode/the-stack/discussions
  included: ''
  intended_uses: creating code LLMs
  license: The Stack is a collection of source code from repositories with various
    licenses. Any use of all or part of the code gathered in The Stack must abide
    by the terms of the original licenses, including attribution clauses when relevant.
    Provenance information is provided for each data point.
  modality: code
  monitoring: ''
  name: The Stack
  nationality: International
  organization: BigCode
  prohibited_uses: See https://huggingface.co/datasets/bigcode/the-stack
  quality_control: allowed users whose data were part of The Stack's training data
    to opt-out
  sample:
  - https://huggingface.co/datasets/bigcode/the-stack/viewer/default/train
  size: 6 TB
  type: dataset
  url: https://arxiv.org/pdf/2211.15533.pdf
- access: open
  analysis: See https://arxiv.org/pdf/2402.19173.pdf
  created_date: 2024-02-28
  dependencies:
  - The Stack v2
  description: StarCoder2-15B model is a 15B parameter model trained on 600+ programming
    languages from The Stack v2, with opt-out requests excluded. The training was
    carried out using the Fill-in-the-Middle objective on 4+ trillion tokens.
  feedback: https://huggingface.co/bigcode/starcoder2-15b/discussions
  intended_uses: The model was trained on GitHub code as well as additional selected
    data sources such as Arxiv and Wikipedia. As such it is not an instruction model
    and commands like "Write a function that computes the square root." do not work
    well. Intended to generate code snippets from given context, but not for writing
    actual functional code directly.
  license: BigCode OpenRail-M
  modality: code; text
  model_card: https://huggingface.co/bigcode/starcoder2-15b
  monitoring: unknown
  name: StarCoder2-15B
  nationality: International
  organization: BigCode
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  quality_control: The model was filtered for permissive licenses and code with no
    license only. A search index is provided to identify where generated code came
    from to apply the proper attribution.
  size: 15B parameters (dense)
  training_emissions: unknown
  training_hardware: 1024 x H100 GPUs
  training_time: unknown
  type: model
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
- access: open
  analysis: See https://arxiv.org/pdf/2402.19173.pdf
  created_date: 2024-02-28
  dependencies:
  - The Stack v2
  description: StarCoder2-7B model is a 7B parameter model trained on 17 programming
    languages from The Stack v2, with opt-out requests excluded. The model uses Grouped
    Query Attention, a context window of 16,384 tokens with a sliding window attention
    of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 3.5+
    trillion tokens.
  feedback: https://huggingface.co/bigcode/starcoder2-7b/discussions
  intended_uses: Intended to generate code snippets from given context, but not for
    writing actual functional code directly. The model has been trained on source
    code from 17 programming languages. The predominant language in source is English
    although other languages are also present. As such the model is capable of generating
    code snippets provided some context but the generated code is not guaranteed to
    work as intended. It can be inefficient and contain bugs or exploits. See the
    paper for an in-depth discussion of the model limitations.
  license: BigCode OpenRail-M
  modality: code; text
  model_card: https://huggingface.co/bigcode/starcoder2-7b
  monitoring: unknown
  name: StarCoder2-7B
  nationality: International
  organization: BigCode
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  quality_control: The model was filtered for permissive licenses and code with no
    license only. A search index is provided to identify where generated code came
    from to apply the proper attribution.
  size: 7B parameters (dense)
  training_emissions: 29,622.83 kgCO2eq
  training_hardware: 432 H100 GPUs
  training_time: 145,152 hours (cumulative)
  type: model
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
- access: open
  analysis: See https://arxiv.org/pdf/2402.19173.pdf
  created_date: 2024-02-28
  dependencies:
  - The Stack v2
  description: StarCoder2-3B model is a 3B parameter model trained on 17 programming
    languages from The Stack v2, with opt-out requests excluded. The model uses Grouped
    Query Attention, a context window of 16,384 tokens with a sliding window attention
    of 4,096 tokens, and was trained using the Fill-in-the-Middle objective on 3+
    trillion tokens.
  feedback: https://huggingface.co/bigcode/starcoder2-3b/discussions
  intended_uses: Intended to generate code snippets from given context, but not for
    writing actual functional code directly. The model has been trained on source
    code from 17 programming languages. The predominant language in source is English
    although other languages are also present. As such the model is capable of generating
    code snippets provided some context but the generated code is not guaranteed to
    work as intended. It can be inefficient and contain bugs or exploits. See the
    paper for an in-depth discussion of the model limitations.
  license: BigCode OpenRail-M
  modality: code; text
  model_card: https://huggingface.co/bigcode/starcoder2-3b
  monitoring: unknown
  name: StarCoder2-3B
  nationality: International
  organization: BigCode
  prohibited_uses: See BigCode Open RAIL-M license and FAQ
  quality_control: The model was filtered for permissive licenses and code with no
    license only. A search index is provided to identify where generated code came
    from to apply the proper attribution.
  size: 3B parameters (dense)
  training_emissions: 16,107.01 kgCO2eq
  training_hardware: 160 A100 GPUs
  training_time: 97,120 hours (cumulative)
  type: model
  url: https://www.servicenow.com/company/media/press-room/huggingface-nvidia-launch-starcoder2.html
