- access: Open
  analysis: Evaluations show that GLM-4, 1) closely rivals or outperforms GPT-4 in
    terms of general metrics such as MMLU, GSM8K, MATH, BBH, GPQA, and HumanEval,
    2) gets close to GPT-4-Turbo in instruction following as measured by IFEval, 3)
    matches GPT-4 Turbo (128K) and Claude 3 for long context tasks, and 4) outperforms
    GPT-4 in Chinese alignments as measured by AlignBench.
  created_date: 2023-07-02
  dependencies: []
  description: ChatGLM is an evolving family of large language models that have been
    developed over time. The GLM-4 language series, includes GLM-4, GLM-4-Air, and
    GLM-4-9B. They are pre-trained on ten trillions of tokens mostly in Chinese and
    English and are aligned primarily for Chinese and English usage. The high-quality
    alignment is achieved via a multi-stage post-training process, which involves
    supervised fine-tuning and learning from human feedback. GLM-4 All Tools model
    is further aligned to understand user intent and autonomously decide when and
    which tool(s) to use.
  feedback: Unknown
  intended_uses: General language modeling, complex tasks like accessing online information
    via web browsing and solving math problems using Python interpreter.
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/THUDM/glm-4-9b
  monitoring: Unknown
  name: ChatGLM
  nationality: unknown
  organization: Team GLM, Zhipu AI, Tsinghua University
  prohibited_uses: Unknown
  quality_control: High-quality alignment is achieved via a multi-stage post-training
    process, which involves supervised fine-tuning and learning from human feedback.
  size: 9B parameters
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://arxiv.org/pdf/2406.12793
