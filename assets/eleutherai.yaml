- access:
    explanation: 'The dataset is freely available to the public and can be downloaded
      from The Eye [[The Pile]](https://mystic.the-eye.eu/public/AI/pile/).

      '
    value: open
  analysis: 'Analyses of the data''s composition, document statistics, language/dialectal
    coverage, topical distribution, and biases are conducted are conducted in the
    paper [[The Pile Paper]](https://arxiv.org/pdf/2101.00027.pdf).

    '
  created_date: 2021-01-01
  datasheet: https://arxiv.org/pdf/2201.07311.pdf
  dependencies: []
  description: 'A large language model training dataset, used to train GPT-NeoX-20B.

    '
  excluded: 'Authors report that they have excluded some datasets "because they were
    too small to be worth spending time or because the English component of the data
    did not merit inclusion on its own. Three datasets were excluded for other reasons:
    (1) US Congressional Records were excluded because it "reflects the opinions and
    biases of the political class over the past 200 years, including segregationism
    and xenophobia." (2) Online Fanfiction resources amounting to Hundreds of GiB
    were excluded on logistical grounds. (3) Literotica, platform where users can
    upload short-form erotic fiction, was excluded because the authors decided to
    exclude fanfiction, the corpus would require significant investigation, and corpus
    contain significant amount of stereotyping [[Appendix B]](https://arxiv.org/pdf/2101.00027.pdf).

    '
  feedback: 'Feedback can be given by emailing the authors at contact at eleuther.ai.

    '
  included: 'The Pile data come from 22 sources, with over half of the data being
    from Common Crawl (Pile-CC; 227GB), fiction and nonfiction books (Books3; 101GB),
    biomedical articles (PubMed Central; 90GB), and code (Github; 95 GB). Refer to
    the paper for full decomposition [[Table 1]](https://arxiv.org/pdf/2101.00027.pdf#table.caption.2).

    '
  intended_uses: 'The Pile was intended to be used as a high quality large text dataset
    for language modeling tasks, explained in more detail in the paper [[Section 1]](https://arxiv.org/pdf/2101.00027.pdf#section.1).

    '
  license:
    explanation: As indicated in the paper.
    value: MIT
  modality: code, text
  monitoring: none
  name: The Pile
  nationality: USA
  organization: EleutherAI
  prohibited_uses: none
  quality_control: 'In addition to the data inclusion and exclusion decisions, the
    quality was controlled through filtering for English (pycld2 language classifier),
    filtering for documents similar to OpenWebText2 (classifier on CommonCrawl), and
    several forms of deduplication as detailed in the paper [[Appendix C]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.C)
    [[Appendix D]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.D).

    '
  sample:
  - "...pot trending topics and the coverage around them. First up, there\u2019s a\
    \ bit of a visual redesign. Previously, clicking on a trending topic would highlight\
    \ a story from one publication, and you\u2019d have to scroll down past a live\
    \ video section to view related stories. Facebook is replacing that system with\
    \ a simple carousel, which does a better job of showing you different coverage\
    \ options. To be clear, the change doesn\u2019t affect how stories are sourced,\
    \ according to Facebook. It\u2019s still the same algorithm pickin..."
  - Total knee arthroplasty (TKA) is a promising treatment for endstage osteoarthritis
    (OA) of the knee for alleviating pain and restoring the function of the knee.
    Some of the cases with bilateral TKA are symptomatic, necessitating revision arthroplasty
    in both the knees. A bilateral revision TKA can be done ei
  - On the converse, the set-valued map $\Phi:[0,3]\rightrightarrows [0,3]$ $$\Phi(x):=\left\{\begin{array}{ll}
    \{1\} & \mbox{ if } 0\leq x<1\\ {}[1,2] & \mbox{ if } 1\leq x\leq 2\\ \{2\} &
  - "This Court thus uses the same interpretation of V.R.C.P. 52(a) as it did *487\
    \ under the previous statutory requirement found in 12 V.S.A. \xA7 2385.  In essense,\
    \ the defendants urge that this Court should reconsider the case of Green Mountain\
    \ Marble Co. v. Highway Board, supra, and follow the Federal practice of looking\
    \ to the evide"
  size: 825 GB
  type: dataset
  url: https://arxiv.org/pdf/2101.00027.pdf
- access:
    explanation: 'The model can be downloaded for free from [[The Eye]](https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500.tar.zstd)

      '
    value: open
  analysis: ''
  created_date:
    explanation: Date model blog post was published
    value: 2021-06-04
  dependencies:
  - The Pile
  description: GPT-J is an open-source autoregressive language model.
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'As indicated in the [[Github repository]](https://github.com/kingoflolz/mesh-transformer-jax)

      '
    value: Apache 2.0
  modality: text; text
  model_card: ''
  monitoring: ''
  name: GPT-J
  nationality: USA
  organization: EleutherAI
  prohibited_uses: ''
  quality_control: ''
  size: 6B parameters (dense)
  training_emissions: ''
  training_hardware: 'TRC (Unspecified # of TPU v3-8s)'
  training_time: ''
  type: model
  url: https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
- access:
    explanation: 'The model can be downloaded for free from [[The Eye]](https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/)

      '
    value: open
  analysis: ''
  created_date:
    explanation: Date Github repo was update
    value: 2021-03-21
  dependencies:
  - The Pile
  description: ''
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: text; text
  model_card: ''
  monitoring: ''
  name: GPT-Neo
  nationality: USA
  organization: EleutherAI
  prohibited_uses: ''
  quality_control: ''
  size: 2.7B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://github.com/EleutherAI/gpt-neo
- access:
    explanation: 'The model can be downloaded for free The Eye [[GPT-NeoX-20B]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/).

      '
    value: open
  analysis: 'The model was evaluated on standard NLP benchmarks: LAMBADA, ANLI, HellaSwag,
    MMLU among others [[Section 4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#section.4).

    '
  created_date: 2022-02-02
  dependencies:
  - The Pile
  description: 'GPT-NeoX (20B) is an open-sourced autoregressive language model.

    '
  feedback: 'Feedback can be provided using the  # 20b channel in EleutherAI Discord
    group [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/). Find
    the Discord link in the FAQ page [[FAQ]](https://www.eleuther.ai/faq/).

    '
  intended_uses: 'As stated in the model card: "GPT-NeoX-20B learns an inner representation
    of the English language that can be used to extract features useful for downstream
    tasks. The model is best at what it was pretrained for however, which is generating
    text from a prompt. Due to the generality of the pretraining set, it has acquired
    the ability to generate completions across a wide range of tasks - from programming
    to fiction writing [[Model Card]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md)."

    '
  license:
    explanation: 'As indicated in the accompanying blog post [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).

      '
    value: Apache 2.0
  modality: text; text, code
  model_card: https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md
  monitoring: none
  name: GPT-NeoX
  nationality: USA
  organization: EleutherAI
  prohibited_uses: none
  quality_control: none
  size: 20B parameters (dense)
  training_emissions:
    explanation: 'The amount of emission during the development and training of the
      model based on the author''s estimation [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).

      '
    value: 31.73 tCO2e
  training_hardware:
    explanation: 'As outline by the authors [[Section 2.3]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.2.3)

      '
    value: 12 x 8 A100 GPUs
  training_time:
    explanation: 'Training time was reported as 1830 hours reported by the authors,
      equaling 76.25 days. [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
      The authors report that 96 (12 * 8) A100 GPUs were used during the training.
      The A100 GPUs have a single precision performance of 0.0195 petaflops [[A100
      Datasheet]](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf).
      Assuming the estimated utilization is 33%, following [[OpenAI AI and Computer
      Blog]](https://openai.com/blog/ai-and-compute/#addendum), the training time
      is 47.10 petaflop/s-day (76.25 * 96 * 0.0195 * 0.33).

      '
    value: 47.10 petaflop/s-day
  type: model
  url: http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf
- access:
    explanation: 'GooseAI API can be accessed by signing up on the goose.ai website.

      '
    value: limited
  adaptation: unknown
  created_date: unknown
  dependencies:
  - GPT-NeoX
  description: 'GooseAI API is an API service providing access to NLP services.

    '
  failures: unknown
  feedback:
    explanation: 'In the "Error Reporting and Feedback" section of the Goose.ai Terms
      of Service, GooseAI asks all the feedback to be sent to support at goose.ai
      [[GooseAI Terms of Service]](https://goose.ai/docs/tos).

      '
    value: Email support
  intended_uses: 'Intended to be used as an NLP infrastructure.

    '
  license:
    explanation: "\"Subject to Customer\u2019s strict compliance with this TOS, GooseAI\
      \ grants Customer a limited, non-exclusive, non-transferable, non-sublicensable,\
      \ revocable license to access and use the Platform as described in and subject\
      \ to this TOS\" - excerpt from the Terms of Service document.\n"
    value: custom
  monitoring:
    explanation: 'In the "GooseAI Monitoring and Enforcement" section of GooseAI''s
      Acceptable Use Policy (AUP), it is stated that Goose.AI has the right to investigate
      any suspected violation of its AUP [[GooseAI Acceptable Use Policy]](https://goose.ai/docs/aup).

      '
    value: At will monitoring by the provider
  monthly_active_users: unknown
  name: GooseAI API
  nationality: USA
  organization: GooseAI
  output_space:
    explanation: 'Question/Answer and Classification tasks are coming soon according
      to GooseAI [[Main Page]](goose.ai).

      '
    value: Text Generation, Text Completion
  prohibited_uses:
    explanation: 'Prohibited uses are detailed in the Acceptable Use Policy [[GooseAI
      Acceptable Use Policy]](https://goose.ai/docs/aup).

      '
    value: 'Illegal or abusive activity, security violations, network abuse

      '
  quality_control: unknown
  terms_of_service: https://goose.ai/docs/tos
  type: application
  url: goose.ai
  user_distribution: unknown
- access: open
  analysis: Evaluated by human testers rating alignment of text input, image output
    pairs.
  created_date: 2022-09-04
  dependencies:
  - VQGAN
  - CLIP
  description: VQGAN-CLIP is a model that better generates and edits images using
    a multimodal encoder to guide image generation.
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: text; image
  model_card: ''
  monitoring: ''
  name: VQGAN-CLIP
  nationality: USA
  organization: EleutherAI
  prohibited_uses: ''
  quality_control: ''
  size: 227M parameters (dense)
  training_emissions: unknown
  training_hardware: 1 NVIDIA Tesla K80 GPU
  training_time: Less than 1 V100-hour
  type: model
  url: https://arxiv.org/pdf/2204.08583.pdf
- access: open
  analysis: Evaluated on a variety of NLP benchmarks and found to perform similarly
    to OPT and BLOOM models.
  created_date: 2023-05-31
  dependencies:
  - The Pile
  description: A suite of 16 LLMs all trained on public data seen in the exact same
    order and ranging in size from 70M to 12B parameters
  feedback: https://huggingface.co/EleutherAI/pythia-6.9b/discussions
  intended_uses: ''
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/EleutherAI/pythia-12b
  monitoring: ''
  name: Pythia
  nationality: unknown
  organization: Eleuther AI
  prohibited_uses: ''
  quality_control: ''
  size: 12B parameters (dense)
  training_emissions: ''
  training_hardware: 64 A100 GPUs
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2304.01373.pdf
- access: open
  analysis: Evaluated on math benchmarks in comparison to general large language models.
  created_date: 2023-10-16
  dependencies:
  - Proof Pile 2
  - Code LLaMA
  description: Llemma is a large language model for mathematics.
  feedback: https://huggingface.co/EleutherAI/llemma_34b/discussions
  intended_uses: ''
  license: LLaMA 2
  modality: text; text
  model_card: https://huggingface.co/EleutherAI/llemma_34b
  monitoring: none
  name: Llemma
  nationality: unknown
  organization: Princeton University, Eleuther AI
  prohibited_uses: ''
  quality_control: ''
  size: 34B parameters (dense)
  training_emissions: unknown
  training_hardware: 256 A100 40GB GPUs
  training_time: 47k A100 hours
  type: model
  url: https://arxiv.org/pdf/2310.10631.pdf
- access: open
  analysis: ''
  created_date: 2023-10-16
  datasheet: https://huggingface.co/datasets/EleutherAI/proof-pile-2
  dependencies:
  - Common Crawl
  - OpenWebMath
  - Arxiv
  - RedPajama-Data
  description: Proof Pile 2 is a corpus for language modeling of mathematics.
  excluded: ''
  feedback: https://huggingface.co/datasets/EleutherAI/proof-pile-2/discussions
  included: scientific papers, web data containing mathematics, mathematical code
  intended_uses: ''
  license: MIT
  modality: text
  monitoring: none
  name: Proof Pile 2
  nationality: unknown
  organization: Princeton University, Eleuther AI
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 55B tokens
  type: dataset
  url: https://arxiv.org/pdf/2310.10631.pdf
- access: open
  analysis: The models were evaluated on SuperGLUE, CodeXGLUE, as well as MMLU and
    Bigbench Hard. Comparisons were made with T5v1.1 and found that Pile-T5 models
    performed better in most conditions.
  created_date: 2024-04-15
  dependencies:
  - The Pile
  - T5x
  - LLaMA
  - umT5
  description: Pile-T5 is a version of the broadly used T5 model, but improved to
    eliminate weaknesses such as the omission of crucial code-related tokens. It utilizes
    LLaMA tokenizer and is trained on the Pile, offering enhancements for finetuning
    on downstream tasks, particularly those involving code.
  feedback: unknown
  intended_uses: The model is aimed at downstream tasks that benefit from the encoder-decoder
    architecture. Particularly useful for tasks involving code.
  license: unknown
  modality: text; text
  model_card: none
  monitoring: unknown
  name: Pile-T5
  nationality: USA
  organization: EleutherAI
  prohibited_uses: unknown
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: 2 million steps
  type: model
  url: https://blog.eleuther.ai/pile-t5/
