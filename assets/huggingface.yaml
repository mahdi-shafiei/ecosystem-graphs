- access: open
  analysis: none
  created_date:
    explanation: The date the model was announced
    value: 2021-12-06
  dependencies: []
  description: CodeParrot is an autoregressive language model trained on code
  feedback: none
  intended_uses: none
  license:
    explanation: No license is explicitly provided for this model.
    value: none
  modality: text; code, text
  model_card: none
  monitoring: none
  name: CodeParrot
  nationality: USA
  organization: HuggingFace
  prohibited_uses: none
  quality_control: none
  size: 1B parameters (dense)
  training_emissions: unknown
  training_hardware: 16 x A100 (40GB)
  training_time: unknown
  type: model
  url: https://twitter.com/lvwerra/status/1467933794699259908
- access: open
  analysis: Evaluated on loss, rewards, logps, and logits rejected and chosen.
  created_date: 2023-10-11
  dependencies:
  - Mistral
  description: Zephyr is a series of language models that are trained to act as helpful
    assistants.
  feedback: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha/discussions
  intended_uses: Educational and research purposes
  license: MIT
  modality: text; text
  model_card: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
  monitoring: none
  name: Zephyr
  nationality: USA
  organization: HuggingFace
  prohibited_uses: none
  quality_control: none
  size: 7B parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha
- access: open
  analysis: Evaluated in comparison to Flamingo and OpenFlamingo on standard benchmarks.
  created_date: 2023-08-22
  dependencies:
  - OBELICS
  - Wikipedia
  - LAION-5B
  - PMD
  description: IDEFICS is an open-access visual language model, based on Flamingo.
  feedback: https://huggingface.co/HuggingFaceM4/idefics-80b-instruct/discussions
  intended_uses: Educational and research purposes
  license:
    explanation: Can be found at https://huggingface.co/HuggingFaceM4/idefics-80b-instruct#license
    value: custom
  modality: image, text; text
  model_card: https://huggingface.co/HuggingFaceM4/idefics-80b-instruct
  monitoring: none
  name: IDEFICS
  nationality: USA
  organization: HuggingFace
  prohibited_uses: none
  quality_control: none
  size: 80B parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://huggingface.co/blog/idefics
- access: open
  analysis: Subset of training dataset evaluated for bias using Data Measurements
    Tool.
  created_date: 2023-08-22
  datasheet: https://huggingface.co/datasets/HuggingFaceM4/OBELICS
  dependencies: []
  description: OBELICS is a dataset consisting of 141 million interleaved image-text
    documents scraped from the web and contains 353 million images.
  excluded: All images for which creators explicitly requested opt-out of AI training.
  feedback: https://huggingface.co/datasets/HuggingFaceM4/OBELICS/discussions
  included: ''
  intended_uses: ''
  license: CC-BY-4.0
  modality: image, text
  monitoring: ''
  name: OBELICS
  nationality: USA
  organization: HuggingFace
  prohibited_uses: ''
  quality_control: Sexual and violent content still present in OBELICS even after
    filtering.
  sample: []
  size: 115B tokens
  type: dataset
  url: https://huggingface.co/blog/idefics
- access: open
  analysis: Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for
    Finnish.
  created_date: 2023-11-03
  dependencies: []
  description: FinGPT is a series of Finnish LLMs trained from scratch.
  feedback: https://huggingface.co/TurkuNLP/gpt3-finnish-13B/discussions
  intended_uses: ''
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/TurkuNLP/gpt3-finnish-13B
  monitoring: unknown
  name: FinGPT
  nationality: unknown
  organization: University of Turku, HuggingFace, National Library of Finland
  prohibited_uses: ''
  quality_control: unknown
  size: 13B parameters (dense)
  training_emissions: unknown
  training_hardware: 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a single
    64-core AMD Trento CPU and 512GB of memory.
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2311.05640.pdf
- access: open
  analysis: Evaluated on in-house benchmark, FIN-bench, adapted from BIG-bench for
    Finnish.
  created_date: 2023-11-03
  dependencies:
  - BLOOM
  description: BLUUMI is a multilingual fine-tuned version of BLOOM.
  feedback: https://huggingface.co/TurkuNLP/bloom-finnish-176b/discussions
  intended_uses: ''
  license:
    explanation: Model card indicates same as license for BLOOM.
    value: BigScience RAIL v1.0
  modality: text; text
  model_card: https://huggingface.co/TurkuNLP/bloom-finnish-176b
  monitoring: unknown
  name: BLUUMI
  nationality: unknown
  organization: University of Turku, HuggingFace, National Library of Finland
  prohibited_uses: ''
  quality_control: unknown
  size: 176B parameters (dense)
  training_emissions: unknown
  training_hardware: 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a single
    64-core AMD Trento CPU and 512GB of memory.
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2311.05640.pdf
- access: open
  analysis: Some seed samples were used in different prompt styles and audiences.
    Less than 1% of files are duplicates after running MinHash deduplication. Contaminated
    samples were removed from each dataset split.
  created_date: 2024-02-22
  datasheet: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia
  dependencies:
  - Mixtral
  description: Cosmopedia is a dataset of synthetic textbooks, blogposts, stories,
    posts, and WikiHow articles generated by Mixtral-8x7B-Instruct-v0.1. The dataset
    contains over 30 million files and 25 billion tokens, making it the largest open
    synthetic dataset to date. It covers a variety of topics, mapping worldwide knowledge
    from Web datasets like RefinedWeb and RedPajama, to generate synthetic content.
  excluded: unknown
  feedback: https://huggingface.co/datasets/HuggingFaceTB/cosmopedia/discussions
  included: ''
  intended_uses: ''
  license: unknown
  modality: text
  monitoring: ''
  name: Cosmopedia v0.1
  nationality: unknown
  organization: Hugging Face
  prohibited_uses: unknown
  quality_control: Measures were taken to reduce redundancy and ensure diversity in
    generated content. A decontamination pipeline was implemented to avoid benchmark
    contamination.
  sample: []
  size: 25B tokens
  type: dataset
  url: none
- access: open
  analysis: The performance of Idefics2 has been evaluated on numerous benchmarks.
    It is top of its class size and competes with much larger models such as LLava-Next-34B
    and MM1-30B-chat.
  created_date: 2024-04-15
  dependencies:
  - The Cauldron
  description: Idefics2 is a general multimodal model that takes as input arbitrary
    sequences of text and images, generating text responses. It has the capability
    to describe visual content, answer questions about images, perform basic arithmetic
    operations, create stories grounded in multiple images, and extract information
    from documents.
  feedback: https://huggingface.co/HuggingFaceM4/idefics2-8b/discussions
  intended_uses: The model can be used for answering questions about images, describing
    visual content, creating stories grounded in multiple images, extracting information
    from documents, and performing basic arithmetic operations.
  license: Apache 2.0
  modality: image, text; text
  model_card: https://huggingface.co/HuggingFaceM4/idefics2-8b
  monitoring: unknown
  name: Idefics2
  nationality: unknown
  organization: Hugging Face
  prohibited_uses: unknown
  quality_control: The quality of the model has been ensured by training it on a mixture
    of openly available datasets and enhancing its OCR capabilities. Further improvements
    include manipulating images in their native resolutions and aspect ratios, better
    pre-trained backbones, and allowing for sub-image splitting.
  size: 8B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://huggingface.co/blog/idefics2
- access: open
  analysis: none
  created_date: 2024-04-15
  datasheet: https://huggingface.co/datasets/HuggingFaceM4/the_cauldron
  dependencies:
    explanation: These are the datasets with the most tokens included; the full list
      of all 50 datasets can be found at https://huggingface.co/datasets/HuggingFaceM4/the_cauldron
    value:
    - LNarratives
    - Rendered Text
    - WebSight
    - DaTikz
  description: The Cauldron is an open compilation of 50 manually-curated datasets
    formatted for multi-turn conversations.
  excluded: ''
  feedback: https://huggingface.co/datasets/HuggingFaceM4/the_cauldron/discussions
  included: ''
  intended_uses: ''
  license: CC BY 4.0
  modality: image, text
  monitoring: unknown
  name: The Cauldron
  nationality: unknown
  organization: Hugging Face
  prohibited_uses: ''
  quality_control: unknown
  sample: []
  size: 50 vision-language datasets
  type: dataset
  url: https://huggingface.co/blog/idefics2
