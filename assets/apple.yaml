---
- type: model
  name: MM1
  organization: Apple
  description: MM1 is a family of multimodal models, including both dense variants
    up to 30B and mixture-of-experts (MoE) variants up to 64B.
  created_date: 2024-03-16
  url: https://arxiv.org/pdf/2403.09611.pdf
  model_card: none
  modality: image, text; text
  analysis: Evaluated on image captioning and visual question answering across many
    benchmarks.
  size: 30B parameters (dense)
  dependencies: []
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access: closed
  license: unknown
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: none
- type: model
  name: OpenELM
  organization: Apple
  description: OpenELM is a family of Open-source Efficient Language Models. It
    uses a layer-wise scaling strategy to efficiently allocate parameters within
    each layer of the transformer model, leading to enhanced accuracy.
  created_date: 2024-04-24
  url: https://machinelearning.apple.com/research/openelm
  model_card: https://huggingface.co/apple/OpenELM-3B-Instruct
  modality: text; text
  analysis: The models were evaluated in terms of zero-shot, LLM360, and OpenLLM
    leaderboard results.
  size: 3B parameters
  dependencies:
    - RefinedWeb
    - The Pile
    - RedPajama-Data
    - Dolma
    - CoreNet library
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: unknown
  access: open
  license: Apple
  intended_uses: To empower and enrich the open research community by providing
    access to state-of-the-art language models.
  prohibited_uses: No explicit prohibited uses stated, though it is noted that users
    should undertake thorough safety testing.
  monitoring: none
  feedback: https://huggingface.co/apple/OpenELM-3B-Instruct/discussions
