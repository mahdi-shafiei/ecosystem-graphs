- access: open
  analysis: Evaluated in comparison to LLaMA series models on standard language benchmarks.
  created_date: 2023-09-27
  dependencies: []
  description: Mistral is a compact language model.
  feedback: https://huggingface.co/mistralai/Mistral-7B-v0.1/discussions
  intended_uses: ''
  license: Apache 2.0
  modality: text; text
  model_card: https://huggingface.co/mistralai/Mistral-7B-v0.1
  monitoring: none
  name: Mistral
  nationality: USA
  organization: Mistral AI
  prohibited_uses: ''
  quality_control: ''
  size: 7.3B parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://mistral.ai/news/announcing-mistral-7b/
- access: limited
  analysis: Evaluated on commonly used benchmarks in comparison to the current LLM
    leaders.
  created_date: 2024-02-26
  dependencies: []
  description: "Mistral Large is Mistral AI\u2019s new cutting-edge text generation\
    \ model."
  feedback: none
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: none
  monitoring: ''
  name: Mistral Large
  nationality: USA
  organization: Mistral AI
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://mistral.ai/news/mistral-large/
- access: limited
  adaptation: ''
  created_date: 2024-02-26
  dependencies:
  - Mistral
  - Mistral Large
  description: Le Chat is a first demonstration of what can be built with Mistral
    models and what can deployed in the business environment.
  failures: unknown
  feedback: none
  intended_uses: ''
  license: unknown
  monitoring: ''
  monthly_active_users: unknown
  name: Le Chat
  nationality: USA
  organization: Mistral AI
  output_space: ''
  prohibited_uses: ''
  quality_control: ''
  terms_of_service: https://mistral.ai/terms/#terms-of-use
  type: application
  url: https://mistral.ai/news/le-chat-mistral/
  user_distribution: unknown
- access: open
  analysis: Performance of Codestral is evaluated in Python, SQL, and additional languages,
    C++, bash, Java, PHP, Typescript, and C#. Fill-in-the-middle performance is assessed
    using HumanEval pass@1 in Python, JavaScript, and Java.
  created_date: 2024-05-29
  dependencies: []
  description: Codestral is an open-weight generative AI model explicitly designed
    for code generation tasks. It helps developers write and interact with code through
    a shared instruction and completion API endpoint. Mastering code and English,
    it can be used to design advanced AI applications for software developers. It
    is fluent in 80+ programming languages.
  feedback: none
  intended_uses: Helps developers write and interact with code, design advanced AI
    applications for software developers, integrated into LlamaIndex and LangChain
    for building applications, integrated in VSCode and JetBrains environments for
    code generation and interactive conversation.
  license: Mistral AI Non-Production License
  modality: text; code
  model_card: none
  monitoring: unknown
  name: Codestral
  nationality: USA
  organization: Mistral AI
  prohibited_uses: unknown
  quality_control: ''
  size: 22B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://mistral.ai/news/codestral/
- access: open
  analysis: The model underwent an advanced fine-tuning and alignment phase. Its performance
    was evaluated using GPT4o as a judge on official references. It was compared to
    recent open-source pre-trained models Gemma 2 9B, Llama 3 8B regarding multilingual
    performance and coding accuracy. Tekken tokenizer's compression ability was compared
    with previous tokenizers like SentencePiece and the Llama 3 tokenizer.
  created_date: 2024-07-18
  dependencies: []
  description: The Mistral NeMo model is a state-of-the-art 12B model built in collaboration
    with NVIDIA, offering a large context window of up to 128k tokens. The model is
    suitable for multilingual applications and exhibits excellent reasoning, world
    knowledge, and coding accuracy. It's easy to use and a drop-in replacement in
    a system that uses Mistral 7B. The model uses a new tokenizer, Tekken, based on
    Tiktoken, which is trained on over 100 languages. It compresses natural language
    text and source code more efficiently than previously used tokenizers.
  feedback: Problems should be reported to the Mistral AI team, though the specific
    method of reporting is unknown.
  intended_uses: The model can be used for multilingual applications, understanding
    and generating natural language as well as source code, handling multi-turn conversations,
    and providing more precise instruction following.
  license: Apache 2.0
  modality: text; text
  model_card: unknown
  monitoring: Unknown
  name: Mistral NeMo
  nationality: USA
  organization: Mistral AI, NVIDIA
  prohibited_uses: Unknown
  quality_control: The model underwent an advanced fine-tuning and alignment phase.
    Various measures such as accuracy comparisons with other models and instruction-tuning
    were implemented to ensure its quality.
  size: 12B parameters
  training_emissions: Unknown
  training_hardware: NVIDIA hardware, specifics unknown
  training_time: Unknown
  type: model
  url: https://mistral.ai/news/mistral-nemo/
- access: open
  analysis: The model has been tested for in-context retrieval capabilities up to
    256k tokens. It has been created with advanced code and reasoning capabilities,
    which enables it to perform on par with SOTA transformer-based models.
  created_date: 2024-07-16
  dependencies: []
  description: Codestral Mamba is a Mamba2 language model that is specialized in code
    generation. It has a theoretical ability to model sequences of infinite length
    and offers linear time inference. This makes it effective for extensive user engagement
    and is especially practical for code productivity use cases. Codestral Mamba can
    be deployed using the mistral-inference SDK or through TensorRT-LLM, and users
    can download the raw weights from HuggingFace.
  feedback: Problems with the model can be reported through the organization's website.
  intended_uses: The model is intended for code generation and can be utilized as
    a local code assistant.
  license: Apache 2.0
  modality: text; text
  model_card: unknown
  monitoring: Unknown
  name: Codestral Mamba
  nationality: USA
  organization: Mistral AI
  prohibited_uses: Unknown
  quality_control: Unknown
  size: 7.3B parameters
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://mistral.ai/news/codestral-mamba/
- access: open
  analysis: The model's performance has been evaluated on the MATH and MMLU industry-standard
    benchmarks. It scored notably higher on both these tests than the base model Mistral
    7B.
  created_date: 2024-07-16
  dependencies:
  - Mistral 7B
  description: "Math\u03A3tral is a 7B model designed for math reasoning and scientific\
    \ discovery. It achieves state-of-the-art reasoning capacities in its size category\
    \ across various industry-standard benchmarks. This model stands on the shoulders\
    \ of Mistral 7B and specializes in STEM subjects. It is designed to assist efforts\
    \ in advanced mathematical problems requiring complex, multi-step logical reasoning.\
    \ It particularly achieves 56.6% on MATH and 63.47% on MMLU."
  feedback: Feedback is likely expected to be given through the HuggingFace platform
    where the model's weights are hosted or directly to the Mistral AI team.
  intended_uses: The model is intended for use in solving advanced mathematical problems
    requiring complex, multi-step logical reasoning or any math-related STEM subjects
    challenges.
  license: Apache 2.0
  modality: text; text
  model_card: unknown
  monitoring: Unknown
  name: "Math\u03A3tral"
  nationality: USA
  organization: Mistral AI
  prohibited_uses: Unknown
  quality_control: This model has been fine-tuned from a base model and its inference
    and performance have been tested on several industry benchmarks.
  size: 7B parameters
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://mistral.ai/news/mathstral/
- access:
    explanation: The model is available under the Mistral Research License (MRL) for
      research and educational use; and the Mistral Commercial License for experimentation,
      testing, and production for commercial purposes.
    value: open
  analysis: We evaluate Pixtral Large against frontier models on a set of standard
    multimodal benchmarks, through a common testing harness.
  created_date: 2024-11-18
  dependencies:
  - Mistral Large 2
  description: Pixtral Large is the second model in our multimodal family and demonstrates
    frontier-level image understanding. Particularly, the model is able to understand
    documents, charts and natural images, while maintaining the leading text-only
    understanding of Mistral Large 2.
  feedback: unknown
  intended_uses: RAG and agentic workflows, making it a suitable choice for enterprise
    use cases such as knowledge exploration and sharing, semantic understanding of
    documents, task automation, and improved customer experiences.
  license:
    explanation: The model is available under the Mistral Research License (MRL) for
      research and educational use; and the Mistral Commercial License for experimentation,
      testing, and production for commercial purposes.
    value: Mistral Research License (MRL), Mistral Commercial License
  modality:
    explanation: Pixtral Large is the second model in our multimodal family and demonstrates
      frontier-level image understanding.
    value: text, image; text
  model_card: unknown
  monitoring: unknown
  name: Pixtral Large
  nationality: USA
  organization: Mistral AI
  prohibited_uses: unknown
  quality_control: unknown
  size:
    explanation: Today we announce Pixtral Large, a 124B open-weights multimodal model.
    value: 124B parameters
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://mistral.ai/news/pixtral-large/
- access:
    explanation: "The API is also available on Google Cloud\u2019s Vertex AI, in private\
      \ preview on Azure AI Foundry, and coming soon to Amazon Bedrock."
    value: closed
  analysis: Benchmarks We have benchmarked the new Codestral with the leading sub-100B
    parameter coding models that are widely considered to be best-in-class for FIM
    tasks.
  created_date: 2025-01-13
  dependencies: []
  description: Lightweight, fast, and proficient in over 80 programming languages,
    Codestral is optimized for low-latency, high-frequency usecases and supports tasks
    such as fill-in-the-middle (FIM), code correction and test generation.
  feedback: "We can\u2019t wait to hear your experience! Try it now Try it on Continue.dev\
    \ with VsCode or JetBrains"
  intended_uses: Highly capable coding companion, regularly boosting productivity
    several times over.
  license: unknown
  modality:
    explanation: it for free in Continue for VS Code or JetBrains
    value: text; text
  model_card: unknown
  monitoring: unknown
  name: Codestral 25.01
  nationality: USA
  organization: Mistral AI
  prohibited_uses: unknown
  quality_control: unknown
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://mistral.ai/news/codestral-2501/
