- access: open
  analysis: Evaluated on all language datasets compared to similarly sized SOTA models,
    with Aurora-M achieving strong performance in most.
  created_date: 2024-04-23
  dependencies:
  - StarCoderPlus
  description: Aurora-M is a 15B parameter multilingual open-source model trained
    on English, Finnish, Hindi, Japanese, Vietnamese, and code.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: text; text
  model_card: none
  monitoring: unknown
  name: Aurora-M
  nationality: unknown
  organization: Tokyo Institute of Technology, MIT-IBM Watson Lab, Sapienza University
    of Rome
  prohibited_uses: ''
  quality_control: ''
  size: 15B parameters
  training_emissions:
    explanation: The training process operated entirely on 100% hydro-powered energy
      and included waste heat recycling.
    value: unknown
  training_hardware: LUMI supercomputer, using 128 AMD MI250X GPUs
  training_time: 48 days
  type: model
  url: https://arxiv.org/pdf/2404.00399
- access:
    explanation: Reflection Llama-3.1 70B is an open-source LLM.
    value: open
  analysis: Unknown
  created_date: 2024-09-28
  dependencies:
  - Glaive
  - Llama 3.1
  description: Reflection Llama-3.1 70B is an open-source LLM, trained with a new
    technique called Reflection-Tuning that teaches a LLM to detect mistakes in its
    reasoning and correct course. The model was trained on synthetic data generated
    by Glaive.
  feedback: Unknown
  intended_uses: The model is intended for complex reasoning and reflection tasks.
    It is designed to separate its internal thoughts and reasoning from its final
    answer.
  license: Llama 3.1 Community License Agreement
  modality:
    explanation: you can sample from Reflection Llama-3.1 70B using the same code,
      pipelines, etc. as any other Llama model. This implies that the model takes
      text input and produces text output.
    value: text; text
  model_card: https://huggingface.co/mattshumer/Reflection-70B
  monitoring: Unknown
  name: Reflection Llama-3.1 70B
  nationality: Unknown
  organization: Unknown
  prohibited_uses: Unknown
  quality_control: The model uses a Reflection-Tuning technique which allows it to
    notice mistakes in its reasoning and correct them.
  size:
    explanation: The size is in the model's name, Reflection Llama-3.1 70B.
    value: 70B parameters
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://huggingface.co/mattshumer/Reflection-70B
- access:
    explanation: Pixtral 12B is released under Apache 2.0 license.
    value: open
  analysis: The model outperforms other models of similar and larger sizes on multimodal
    benchmarks. An open-source benchmark, MM-MT-Bench, is contributed for evaluating
    vision-language models.
  created_date: 2024-10-10
  dependencies:
  - Mistral Nemo 12B
  description: Pixtral 12B is a 12-billion-parameter multimodal language model trained
    to understand both natural images and documents, achieving leading performance
    on various multimodal benchmarks without compromising on natural language performance.
  feedback: Unknown
  intended_uses: Multimodal instruction following tasks, capable of multi-turn, multi-image
    conversations.
  license:
    explanation: Pixtral 12B is released under Apache 2.0 license.
    value: Apache 2.0
  modality:
    explanation: Pixtral 12B is trained to understand both natural images and documents.
    value: text; image
  model_card: unknown
  monitoring: Evaluation protocols and benchmarks are open-sourced to establish fair
    and standardized testing.
  name: Pixtral 12B
  nationality: Unknown
  organization: Unknown
  prohibited_uses: Unknown
  quality_control: Evaluation protocols for multimodal language models were standardized
    and analysis was conducted to improve the reliability of model evaluations.
  size:
    explanation: "We introduce Pixtral 12B, a 12\u2013billion-parameter multimodal\
      \ language model."
    value: 12B parameters
  training_emissions: Unknown
  training_hardware: Unknown
  training_time: Unknown
  type: model
  url: https://arxiv.org/pdf/2410.07073
