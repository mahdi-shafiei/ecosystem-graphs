- access:
    explanation: 'Neither the 8.3B parameter model trained to convergence nor the
      1 trillion paramter model is available for download

      '
    value: closed
  analysis: ''
  created_date:
    explanation: The date the paper for the 1 trillion parameter model was published
    value: 2021-04-09
  dependencies: []
  description: Megatron-LM is an autoregressive language model
  feedback: none
  intended_uses: none
  license: unknown
  modality: text; text
  model_card: none
  monitoring: none
  name: Megatron-LM
  nationality: USA
  organization: NVIDIA
  prohibited_uses: none
  quality_control: unknown
  size: 1T parameters (dense)
  training_emissions: unknown
  training_hardware: 3072 A100 GPUs
  training_time: 84 days
  type: model
  url: https://arxiv.org/abs/2104.04473
- access: open
  analysis: ''
  created_date: 2022-06-17
  datasheet: ''
  dependencies:
  - YouTube
  - Wikipedia
  - Reddit
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: MIT
  modality: text, video
  monitoring: ''
  name: MineDojo
  nationality: USA
  organization: NVIDIA
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 730k videos, 6k Wikipedia pages, 340k reddit posts
  type: dataset
  url: https://arxiv.org/abs/2206.08853
- access: open
  analysis: ''
  created_date: 2022-10-06
  datasheet: ''
  dependencies:
  - T5
  - Mask R-CNN
  - VIMA dataset
  description: ''
  excluded: ''
  feedback: ''
  included: ''
  intended_uses: ''
  license: MIT
  modality: image, text
  monitoring: ''
  name: VIMA dataset
  nationality: unknown
  organization: NVIDIA, Stanford
  prohibited_uses: ''
  quality_control: ''
  sample: []
  size: 200M parameters (dense model)
  type: dataset
  url: https://vimalabs.github.io/
- access: open
  analysis: ''
  created_date: 2022-10-06
  dependencies: []
  description: ''
  feedback: ''
  intended_uses: ''
  license: MIT
  modality: image, text; robotics trajectories
  model_card: ''
  monitoring: ''
  name: VIMA
  nationality: unknown
  organization: NVIDIA, Stanford
  prohibited_uses: ''
  quality_control: ''
  size: 200M parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://vimalabs.github.io/
- access: open
  analysis: Evaluated on standard LLM benchmarks across a range of fields like reasoning,
    code generation, and mathematical skills.
  created_date: 2024-02-27
  dependencies: []
  description: Nemotron 4 is a 15-billion-parameter large multilingual language model
    trained on 8 trillion text tokens.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: text; code, text
  model_card: none
  monitoring: unknown
  name: Nemotron 4
  nationality: USA
  organization: Nvidia
  prohibited_uses: ''
  quality_control: Deduplication and quality filtering techniques are applied to the
    training dataset.
  size: 15B parameters (dense)
  training_emissions: unknown
  training_hardware: 3072 H100 80GB SXM5 GPUs across 384 DGX H100 nodes
  training_time: 13 days
  type: model
  url: https://arxiv.org/pdf/2402.16819.pdf
