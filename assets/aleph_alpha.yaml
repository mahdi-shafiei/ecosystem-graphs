---
- type: dataset
  name: Luminous dataset
  organization: Aleph Alpha
  description: The dataset used to train the Luminous models.
  created_date:
    explanation: Date the models were announced by the Aleph Alpha twitter account
    value: 2022-04-14
  url: none
  datasheet: none
  modality: text
  size: unknown
  sample: []
  analysis: unknown
  dependencies: []
  included: unknown
  excluded: unknown
  quality_control: unknown
  access: closed
  license: unknown
  intended_uses: unknown
  prohibited_uses: unknown
  monitoring: none
  feedback: none
- type: model
  name: Luminous
  organization: Aleph Alpha
  description: Luminous is a family of multilingual language models
  created_date:
    explanation: Date the models were announced by the Aleph Alpha twitter account
    value: 2022-04-14
  url: https://twitter.com/Aleph__Alpha/status/1514576711492542477
  model_card: ''
  modality: text; text
  analysis: ''
  size: 200B parameters (dense)
  dependencies: [Luminous dataset]
  training_emissions: unknown
  training_time: unknown
  training_hardware: unknown
  quality_control: ''
  access:
    explanation: Paid API product
    value: limited
  license: none
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: application
  name: Aleph Alpha API
  organization: Aleph Alpha
  description: The Aleph Alpha API serves a family of text-only language models
    (Luminous) and multimodal text-and-image models (Magma).
  created_date:
    explanation: "Date the Aleph Alpha Python API client was first available based\
      \ on to the [PyPi package history](https://pypi.org/project/aleph-alpha-client/1.0.0/)\n"
    value: 2021-09-30
  url: https://www.aleph-alpha.com/
  dependencies: [Luminous]
  adaptation: ''
  output_space: The text models provide text outputs given text inputs. The multimodal
    models provide text completions given text and image inputs.
  quality_control: ''
  access:
    explanation: The API is a paid product. Refer to the [[API documentation]](https://docs.aleph-alpha.com/docs/introduction/luminous/)
      for further details.
    value: limited
  license:
    explanation: No license was found.
    value: none
  terms_of_service: https://www.aleph-alpha.com/terms-conditions
  intended_uses: unknown
  prohibited_uses: unknown
  monitoring: unknown
  feedback: unknown
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown
- type: model
  name: MAGMA
  organization: Aleph Alpha
  description: An autoregressive VL model that is able to generate text from an
    arbitrary combination of visual and textual input
  created_date: 2022-10-24
  url: https://arxiv.org/pdf/2112.05253.pdf
  model_card: ''
  modality: image, text; text
  analysis: Evaluated on the OKVQA benchmark as a fully open-ended generative task.
  size: 6B parameters (dense)
  dependencies: [GPT-J, CLIP]
  training_emissions: ''
  training_time: ''
  training_hardware: 32 A100 GPUs
  quality_control: ''
  access: open
  license: MIT
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
- type: model
  name: Pharia-1-LLM-7B
  organization: Aleph Alpha
  description: Pharia-1-LLM-7B is a model that falls within the Pharia-1-LLM model
    family. It is designed to deliver short, controlled responses that match the
    performance of leading open-source models around 7-8 billion parameters. The
    model is culturally and linguistically tuned for German, French, and Spanish
    languages. It is trained on carefully curated data in line with relevant EU
    and national regulations. The model shows improved token efficiency and is particularly
    effective in domain-specific applications, especially in the automotive and
    engineering industries. It can also be aligned to user preferences, making it
    appropriate for critical applications without the risk of shut-down behaviour.
  created_date: 2024-09-08
  url: https://aleph-alpha.com/introducing-pharia-1-llm-transparent-and-compliant/#:~:text=Pharia%2D1%2DLLM%2D7B
  model_card: unknown
  modality: Text; text
  analysis: Extensive evaluations were done with ablation experiments performed
    on pre-training benchmarks such as lambada, triviaqa, hellaswag, winogrande,
    webqs, arc, and boolq. Direct comparisons were also performed with applications
    like GPT and Llama 2.
  size: 7B parameters
  dependencies: [GPT, Llama 2]
  training_emissions: Unknown
  training_time: Unknown
  training_hardware: Unknown
  quality_control: The model comes with additional safety guardrails via alignment
    methods to ensure safe usage. Training data is carefully curated to ensure compliance
    with EU and national regulations.
  access: Open
  license: Aleph Open
  intended_uses: The model is intended for use in domain-specific applications,
    particularly in the automotive and engineering industries. It can also be tailored
    to user preferences.
  prohibited_uses: Unknown
  monitoring: Unknown
  feedback: Feedback can be sent to support@aleph-alpha.com.
