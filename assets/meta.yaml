- access:
    explanation: Models are available for download from [[GitHub repository]](https://github.com/facebookresearch/esm#available-models)
    value: open
  analysis: ''
  created_date:
    explanation: The date the [[model paper]](https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2.full.pdf+html)
      was released
    value: 2022-10-31
  dependencies:
  - UniRef50
  - UniRef90
  description: ESM-2 is a series of protein language models trained on protein sequences
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'The license is provided in the [[Github repository]](https://github.com/facebookresearch/esm#available-models)

      '
    value: MIT
  modality: text; protein sequence
  model_card: none
  monitoring: ''
  name: ESM-2
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 15B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://www.biorxiv.org/content/10.1101/2022.07.20.500902v2.full.pdf+html
- access: closed
  analysis: none
  created_date:
    explanation: The date the model paper was released
    value: 2021-12-08
  datasheet: none
  dependencies:
  - COCO
  - YFCC100M
  - SBU Captions
  - Localized Narratives
  - Visual Genome
  - Wikipedia
  - Conceptual Captions
  - Red Caps
  description: PMD (Public Multimodal Datasets) is a collection of image-text datasets
    introduced in the FLAVA work.
  excluded: YFCC100M is filtered for non-English captions and very short (< 2 word)
    captions.
  feedback: none
  included: none
  intended_uses: unknown
  license: unknown
  modality: image, text
  monitoring: none
  name: PMD
  nationality: USA
  organization: Meta
  prohibited_uses: unknown
  quality_control: Beyond filtering mentioned in excluded, nothing further is done.
  sample: []
  size: 70M
  type: dataset
  url: https://arxiv.org/abs/2112.04482
- access:
    explanation: 'Model checkpoints are available for download from the [[HuggingFace
      repository]](https://huggingface.co/facebook/flava-full)

      '
    value: open
  analysis: FLAVA is benchmarked on a range of vision-only (e.g. CIFAR-10), language-only
    (e.g. GLUE), and multimodal (e.g. Hateful Memes) standard evaluations.
  created_date:
    explanation: The date the model paper was released
    value: 2021-12-08
  dependencies:
  - PMD
  description: FLAVA is a multimodal model composed of an image encoder, text encoder,
    and multimodal encoder.
  feedback: https://huggingface.co/facebook/flava-full/discussions
  intended_uses: 'Per the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full),
    "The model is intended to serve as a reproducible research artifact for research
    communities in the light of models whose exact reproduction details are never
    released such as CLIP and SimVLM."

    '
  license:
    explanation: 'The license is provided in the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full)

      '
    value: BSD-3-Clause
  modality: image, text
  model_card: https://huggingface.co/facebook/flava-full
  monitoring: none
  name: FLAVA
  nationality: USA
  organization: Meta
  prohibited_uses: 'Per the [[HuggingFace repository]](https://huggingface.co/facebook/flava-full),
    "Any deployed use case of the model - whether commercial or not" - is currently
    out of scope.

    '
  quality_control: FLAVA introduces a variety of new modeling techniques, specifically
    with an interest in improved text-image alignment through contrastive objectives.
  size:
    explanation: '110M (Language encoder) + 86M (Vision encoder) + 110M (mul encoder)

      '
    value: 306M
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/abs/2112.04482
- access: closed
  analysis: none
  created_date:
    explanation: 'The date the Galactica paper was released

      '
    value: 2022-11-15
  datasheet: none
  dependencies:
  - CommonCrawl
  - Wikipedia
  - arXiv
  description: The Galactica Corpus is a collection of scientific datasets introduced
    in the Galactica work.
  excluded: ''
  feedback: none
  included: Prompts and reasoning data is explicitly included to improve model capabilities
    derived from this data.
  intended_uses: unknown
  license: unknown
  modality: text
  monitoring: none
  name: The Galactica Corpus
  nationality: USA
  organization: Meta
  prohibited_uses: unknown
  quality_control: ''
  sample: []
  size: 106B tokens
  type: dataset
  url: https://galactica.org/static/paper.pdf
- access:
    explanation: Model checkpoints freely available at https://github.com/paperswithcode/galai
    value: open
  analysis: ''
  created_date:
    explanation: 'The date the Galactica paper was released

      '
    value: 2022-11-15
  dependencies:
  - The Galactica Corpus
  description: Galactica is a family of autoregressive language models.
  feedback: ''
  intended_uses: ''
  license:
    explanation: https://github.com/paperswithcode/galai/blob/main/LICENSE-MODEL.md
    value: CC BY-NC 4.0
  modality: code, text; code, text
  model_card: https://huggingface.co/facebook/galactica-6.7b
  monitoring: ''
  name: Galactica
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 120B parameters (dense)
  training_emissions: unknown
  training_hardware: Meta AI Cluster. Trained on 1024 80GB A100 GPUs (128 8xA100 80GB
    nodes)
  training_time: unknown
  type: model
  url: https://galactica.org/static/paper.pdf
- access:
    explanation: Model weights are available via the [[HuggingFace repository]](https://huggingface.co/facebook/incoder-6B)
    value: open
  analysis: none
  created_date:
    explanation: The date the model paper was released
    value: 2022-04-12
  dependencies: []
  description: InCoder is a language model trained on code with a causal masking objective
  feedback: ''
  intended_uses: ''
  license:
    explanation: The license is provided in the [[HuggingFace repository]](https://huggingface.co/facebook/incoder-6B?text=My+name+is+Lewis+and+I+like+to)
    value: CC BY-NC 4.0
  modality: text; code
  model_card: none
  monitoring: ''
  name: InCoder
  nationality: unknown
  organization: Meta, CMU, TTI-Chicago, UC Berkeley, University of Washington
  prohibited_uses: ''
  quality_control: unknown
  size: 6B parameters (dense)
  training_emissions: Unknown
  training_hardware: 248 V100 GPUs, according to [[the paper]](https://arxiv.org/pdf/2204.05999.pdf)
  training_time: 24 days, according to [[the paper]](https://arxiv.org/pdf/2204.05999.pdf)
  type: model
  url: https://arxiv.org/abs/2204.05999
- access:
    explanation: The 175B model requires manual approval from Meta to access. Other
      models are available through HuggingFace.
    value: limited
  analysis: ''
  created_date:
    explanation: 'The date the OPT paper was submitted to Arxiv

      '
    value: 2022-05-01
  dependencies:
  - RoBERTa dataset
  - The Pile
  - PushShift.io Reddit
  description: OPT is a family of autoregressive language models.
  feedback: ''
  intended_uses: ''
  license:
    explanation: 'All released with the [[OPT-175B License]](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md),
      except 66B (TBD) and 17B (requires manual approval)

      '
    value: OPT-175B License
  modality: text; text
  model_card: https://arxiv.org/pdf/2205.01068.pdf
  monitoring: ''
  name: OPT
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 175B parameters (dense)
  training_emissions:
    explanation: 'Estimate by authors for the OPT-175B model only. Not including ablations
      and baselines.

      '
    value: 75 tCO2e
  training_hardware: Meta AI cluster. Trained on 992 80GB A100 GPUs
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2205.01068
- access:
    explanation: 'The datasets involved are public, but the full dataset is not directly
      available, nor are filtering scripts.

      '
    value: limited
  analysis: ''
  created_date:
    explanation: 'The date that Make-A-Video was posted to arXiv [[arXiv]] (https://arxiv.org/abs/2209.14792).

      '
    value: 2022-09-29
  datasheet: none
  dependencies:
  - LAION-5B
  - WebVid-10M
  - HD-VILA-100M
  description: 'The Make-A-Video dataset is the dataset used to train Make-A-Video,
    which includes both image-text and video-only datasets with specific and significant
    filtering.

    '
  excluded: 'The LAION-5B dataset is filtered to 2.3B by removing NSFW images using
    [https://github.com/GantMan/nsfw](https://github.com/GantMan/nsfw), toxic words
    in text, and images with watermark probability > 0.5. The HD-VILA-100M is randomly
    subsampled to 10M video clips.

    '
  feedback:
    explanation: 'No feedback mechanism is mentioned by the authors.

      '
    value: none
  included:
    explanation: 'Data from the three underlying datasets is filtered, but nothing
      is included beyond this.

      '
    value: none
  intended_uses: unknown
  license:
    explanation: 'No license was found, though the underlying datasets are public
      and have licenses.

      '
    value: none
  modality: image, text, video
  monitoring:
    explanation: 'There is no information on how Meta is internally monitoring the
      use of the dataset.

      '
    value: unknown
  name: Make-A-Video dataset
  nationality: USA
  organization: Meta
  prohibited_uses: unknown
  quality_control: 'The authors exclude NSFW, toxic, and likely watermarked data from
    LAION-5B.

    '
  sample: []
  size: 20M video clips, 2.3B image-text pairs
  type: dataset
  url: https://arxiv.org/pdf/2209.14792.pdf
- access:
    explanation: 'The model has not been released; a form existed to potentially acquire
      access but is now closed as of 2022-12-07 [[Access Form]](https://docs.google.com/forms/u/0/d/e/1FAIpQLSfMjC57wcXWUDV0UbS2Tn6VhjLEiCXaHvWZuWgWRa-Zx8-Few/closedform).

      '
    value: closed
  analysis: 'Model performance was evaluated using automated (Frechet Video Distance;
    Frechet Inception Distance) and human evaluation on two datasets (UCF-101, MSR-VTT)
    in the zero-shot setting.

    '
  created_date:
    explanation: 'The date that Make-A-Video was posted to arXiv [[arXiv]] (https://arxiv.org/abs/2209.14792).

      '
    value: 2022-09-29
  dependencies:
  - Make-A-Video dataset
  description: 'Make-A-Video is a model for Text-to-Video Generation without Text-Video
    Data.

    '
  feedback: none
  intended_uses: unknown
  license: none
  modality: text; video
  model_card: none
  monitoring: unknown
  name: Make-A-Video
  nationality: USA
  organization: Meta
  prohibited_uses: unknown
  quality_control:
    explanation: 'Authors do not report specific quality control steps taken in modeling,
      though filtering is done in producing the Make-A-Video dataset.

      '
    value: none
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2209.14792.pdf
- access: open
  analysis: ''
  created_date: 2023-02-24
  dependencies:
  - CommonCrawl
  - C4
  - Github
  - Wikipedia
  - BooksCorpus
  - arXiv
  - StackExchange
  description: LLaMA is a collection of foundation language models ranging from 7B
    to 65B parameters trained our on trillions of tokens. The LLaMA models show that
    it is possible to train state-of-the-art models using publicly available datasets
    exclusively, without resorting to proprietary and inaccessible datasets.
  feedback: ''
  intended_uses: ''
  license: LLaMa License (model weights), GPLv3 (code)
  modality: text; text
  model_card: ''
  monitoring: ''
  name: LLaMA
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 65B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2302.13971
- access: open
  analysis: Evaluated on standard academic benchmarks and internal Meta libraries.
  created_date: 2023-07-18
  dependencies: []
  description: Llama 2 is an updated version of LLaMA trained on a new mix of publicly
    available data.
  feedback: ''
  intended_uses: Llama 2 is intended for commercial and research use in English. Tuned
    models are intended for assistant-like chat, whereas pretrained models can be
    adapted for a variety of natural language generation tasks.
  license:
    explanation: The license can be found at https://ai.meta.com/resources/models-and-libraries/llama-downloads/
    value: custom
  modality: text; text
  model_card: Can be found at appendix of paper at https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/
  monitoring: ''
  name: Llama 2
  nationality: USA
  organization: Meta
  prohibited_uses: Use in any manner that violates applicable laws or regulations
    (including trade compliance laws). Use in languages other than English. Use in
    any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement
    for Llama 2.
  quality_control: ''
  size: 70B parameters (dense)
  training_emissions: 539 tCO2eq
  training_hardware: NVIDIA A100-80GB GPUs (TDP of 350-400W)
  training_time: ''
  type: model
  url: https://ai.meta.com/resources/models-and-libraries/llama/
- access: open
  analysis: ''
  created_date: 2022-12-22
  dependencies:
  - OPT
  - OPT-IML Bench
  description: ''
  feedback: ''
  intended_uses: ''
  license: OPT-IML 175B License
  modality: text; text
  model_card: ''
  monitoring: ''
  name: OPT-IML
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 175B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://arxiv.org/abs/2212.12017
- access:
    explanation: 'The full dataset can be downloaded at [[SA-1B Download]](https://ai.facebook.com/datasets/segment-anything-downloads/).
      A 50k image preview of the full dataset is available [[here]](https://segment-anything.com/dataset/index.html).

      '
    value: open
  analysis: ''
  created_date:
    explanation: The date the [[Meta blog post]](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)
      was released.
    value: 2023-04-05
  datasheet:
    explanation: Datasheet can be found in the Appendix section of the Segment Anything
      paper.
    value: https://arxiv.org/pdf/2304.02643.pdf#page=25
  dependencies: []
  description: 'SA-1B (Segment Anything 1 Billion) is a dataset designed for training
    general-purpose object segmentation models from open world images. It consists
    of 11M diverse, high-resolution, privacy protecting images and 1.1B high-quality
    segmentation masks.

    '
  excluded:
    explanation: See [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25)
    value: '"We withheld ~2k randomly selected images for testing purposes."  "Each
      image is accompanied by a short caption that describes the content and place
      of the photo in a free form text. Per our agreement with the photo provider
      we are not allowed to release these captions."

      '
  feedback: Feedback can be given via the feedback form on their website [segment-anything.com](https://segment-anything.com/)
    or by emailing at segment-anything at meta.com.
  included:
    explanation: According to section [[Segment Anything Dataset]](https://arxiv.org/pdf/2304.02643.pdf#section.5)
      of the paper and [[SA-1B website]](https://ai.facebook.com/datasets/segment-anything/).
    value: "SA-1B consists of 11M diverse, high-resolution (averaging 1500\xD72250\
      \ pixels), and privacy protecting images collected and licensed from a third\
      \ party photo company. The images are photos taken from a camera, i.e. not artwork.\
      \ The images vary in subject matter. Common themes of the images include: locations,\
      \ objects, scenes. The dataset includes 1.1B high-quality segmentation masks\
      \ collected with the Segment Anything Data Engine. SA-1B only includes automatically\
      \ generated masks (99.1%), as the authors conclude after experiments that the\
      \ automatic masks are high quality and effective for training models. The masks\
      \ range from large scale objects such as buildings to fine grained details such\
      \ as door handles. Masks are provided in the COCO run-length encoding (RLE)\
      \ annotation format.\n"
  intended_uses:
    explanation: See [[SA-1B website]](https://ai.facebook.com/datasets/segment-anything/)
    value: SA-1B is intended to be used for research purposes only. It allows access
      to a privacy protecting and copyright friendly large-scale image dataset. Researchers
      can use it to train and evaluate generic object segmentation models.
  license:
    explanation: SA-1B is released under a favorable license agreement for certain
      research uses and with protections for researchers. See [[SA-1B Dataset Research
      License]](https://ai.facebook.com/datasets/segment-anything-downloads/).
    value: SA-1B Dataset Research License
  modality: image
  monitoring:
    explanation: See [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25)
    value: 'The dataset will be hosted at https://ai.facebook.com/datasets/segment-anything
      and maintained by Meta AI. "If a user observes objectionable image(s) in the
      dataset, we invite them to report the image(s) at segment-anything at meta.com
      for removal" "To aid reproducibility of research using SA-1B, the only updates
      (to the dataset) will be to remove reported images." "We encourage users to
      gather further annotations for SA-1B. Any users who generate annotations will
      be liable for hosting and distributing their annotations."

      '
  name: SA-1B
  nationality: USA
  organization: Meta
  prohibited_uses:
    explanation: See [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25)
    value: "Authors note the following limitations of the dataset:\n  The masks are\
      \ generated by a segmentation model, so there may be errors\nor inconsistencies\
      \ in the masks.\n  While no two images are the same, there are instances of\
      \ images of the same\nsubject taken close together in time.\n  The dataset contains\
      \ scenes of protests, or other gatherings that may suggest\nreligious beliefs,\
      \ political opinions or union memberships that may be offensive.\n"
  quality_control:
    explanation: According to sections [[Segment Anything Dataset]](https://arxiv.org/pdf/2304.02643.pdf#section.5)
      and [[Datasheet]](https://arxiv.org/pdf/2304.02643.pdf#page=25) of the paper.
    value: "- Dataset quality:\n  Due to potential accessibility and storage challenges,\
      \ the original high-resolution images (averaging 3300\xD74950 pixels) were downsampled\
      \ to an average resolution of 1500\xD72250 pixels. Authors note that despite\
      \ the downsampling, the images remain significantly higher in resolution than\
      \ those in many existing vision datasets, such as COCO, where images are typically\
      \ around 480\xD7640 pixels.\n  The images were processed to blur faces and license\
      \ plates to protect the identities of those in the image.\n  To estimate the\
      \ quality of the masks in the images, a random sample of 500 images (\u223C\
      50k masks) was taken and professional annotators were asked to improve the quality\
      \ of all masks in those images.\n- Safety measures:\n  Authors implemented two\
      \ safety measures to prevent objectionable content:\n    (1) Photos are licensed\
      \ from a photo provider and had to meet the terms of service of the photo provider.\
      \ Authors requested that all objectionable content be filtered from the images\
      \ they licensed.\n    (2) Users who observe objectionable images in the dataset\
      \ are invited to report them for removal at segment-anything@meta.com.\n  Despite\
      \ these measures, they observed that a small portion of images contain scenes\
      \ of protests or other gatherings that focus on a diverse spectrum of religious\
      \ beliefs or political opinions that may be considered offensive. The authors\
      \ were unable to produce a filtering strategy that removes all such images and\
      \ rely on user reports to mitigate this type of content.\n"
  sample: []
  size: 11M images, 1.1B mask annotations
  type: dataset
  url: https://ai.facebook.com/datasets/segment-anything/
- access:
    explanation: 'Inference code and model checkpoints are available on the model''s
      [[GitHub repository]](https://github.com/facebookresearch/segment-anything).
      Its training dataset SA-1B can be used for research purposes and is available
      for download [here](https://ai.facebook.com/datasets/segment-anything-downloads/).

      '
    value: open
  analysis:
    explanation: See [[Zero-Shot Transfer Experiments]](https://arxiv.org/pdf/2304.02643.pdf#section.7)
      for more details.
    value: '"We extensively evaluate SAM. First, using a diverse new suite of 23 segmentation
      datasets, we find that SAM produces high-quality masks from a single foreground
      point, often only slightly below that of the manually annotated ground truth.
      Second, we find consistently strong quantitative and qualitative results on
      a variety of downstream tasks under a zero-shot transfer protocol using prompt
      engineering, including edge detection, object proposal generation, instance
      segmentation, and a preliminary exploration of text-to-mask prediction."

      '
  created_date:
    explanation: The date the [[Meta blog post]](https://ai.facebook.com/blog/segment-anything-foundation-model-image-segmentation/)
      was released.
    value: 2023-04-05
  dependencies:
  - SA-1B
  description: SAM (Segment Anything Model) is a foundation model for image segmentation.
    The model is designed and trained to be promptable, and supports flexible prompts
    (point, box, mask and free-form text) to compute masks in real-time to allow interactive
    use.
  feedback: Feedback can be given via the feedback form on their website [segment-anything.com](https://segment-anything.com/)
    or by emailing at segment-anything at meta.com.
  intended_uses:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: '"SAM is intended to be used for any prompt-based segmentation task. We
      explored its use in segmenting objects from a point, edge detection, segmenting
      all objects, and segmenting detected objects. We explored how SAM can integrate
      with other vision models to segment objects from text."

      '
  license:
    explanation: See [[LICENSE]](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE)
    value: Apache 2.0
  modality: image, text; image
  model_card:
    explanation: Model card can be found in the Appendix section of the paper.
    value: https://arxiv.org/pdf/2304.02643.pdf#page=28
  monitoring: ''
  name: SAM
  nationality: USA
  organization: Meta
  prohibited_uses:
    explanation: See [[Discussion]](https://arxiv.org/pdf/2304.02643.pdf#section.8)
    value: "For out-of-scope use cases see terms of use in [[LICENSE]](https://github.com/facebookresearch/segment-anything/blob/main/LICENSE).\
      \ Authors also discuss the following limitations of the model: \"While SAM performs\
      \ well in general, it is not perfect. It can miss fine structures, hallucinates\
      \ small disconnected components at times, and does not produce boundaries as\
      \ crisply as more computationally intensive methods that \u201Czoom-in\u201D\
      , e.g. [18]. In general, we expect dedicated interactive segmentation methods\
      \ to outperform SAM when many points are provided, e.g. [67]. Unlike these methods,\
      \ SAM is designed for generality and breadth of use rather than high IoU interactive\
      \ segmentation. Moreover, SAM can process prompts in real-time, but nevertheless\
      \ SAM's overall performance is not real-time when using a heavy image encoder.\
      \ Our foray into the text-to-mask task is exploratory and not entirely robust,\
      \ although we believe it can be improved with more effort. While SAM can perform\
      \ many tasks, it is unclear how to design simple prompts that implement semantic\
      \ and panoptic segmentation. Finally, there are domain-specific tools, such\
      \ as [7], that we expect to outperform SAM in their respective domains.\"\n"
  quality_control:
    explanation: See [[Segment Anything RAI Analysis]](https://arxiv.org/pdf/2304.02643.pdf#section.6)
      for more details.
    value: '"We perform a Responsible AI (RAI) analysis of our work by investigating
      potential fairness concerns and biases when using SA-1B and SAM. We focus on
      the geographic and income distribution of SA-1B and fairness of SAM across protected
      attributes of people."

      '
  size: unknown
  training_emissions:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: 2.8 metric tons of carbon dioxide
  training_hardware:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: 256 A100 GPUs
  training_time:
    explanation: See [[Model card]](https://arxiv.org/pdf/2304.02643.pdf#page=28)
    value: 68 hours
  type: model
  url: https://arxiv.org/pdf/2304.02643.pdf
- access: closed
  analysis: Evaluated on zero-shot text-to-speech benchmarks, with Voicebox outperforming
    the current state-of-the-art English model VALL-E.
  created_date: 2023-06-16
  dependencies: []
  description: Voicebox is the first generative AI model for speech to generalize
    across tasks with state-of-the-art performance.
  feedback: ''
  intended_uses: ''
  license: ''
  modality: audio; text
  model_card: ''
  monitoring: ''
  name: Voicebox
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 330M parameters (dense)
  training_emissions: unknown
  training_hardware: 32 GPUs of unspecified type
  training_time: 750,000 iterations
  type: model
  url: https://research.facebook.com/publications/voicebox-text-guided-multilingual-universal-speech-generation-at-scale/
- access: open
  analysis: PEER is evaluated on core research questions intended to gauge language
    understanding, proper use of citations, instruction following, and iterative use.
  created_date: 2022-08-24
  dependencies: []
  description: PEER is a collaborative language model that is trained to imitate the
    entire writing process itself. PEER can write drafts, add suggestions, propose
    edits and provide explanations for its actions.
  feedback: ''
  intended_uses: adapting LLMs to work with collaborative writing and updating.
  license: ''
  modality: text; text
  model_card: ''
  monitoring: ''
  name: PEER
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: Heuristics and edit filtering was used on data set, which consisted
    mostly of Wikipedia pages.
  size: 3B parameters (dense)
  training_emissions: ''
  training_hardware: 64 GPUs
  training_time: ''
  type: model
  url: https://arxiv.org/pdf/2208.11663.pdf
- access: open
  analysis: MusicGen was evaluated on standard music benchmarks of Frechet Audio Distance,
    Kullback-Leibler Divergence, and its CLAP score.
  created_date: 2023-08-02
  dependencies:
  - Meta Music Initative Sound Collection
  - Shutterstock music collection
  - Pond5 music collection
  description: MusicGen is a simple and controllable model for music generation that
    doesn't require self-supervised semantic representation
  feedback: https://huggingface.co/spaces/facebook/MusicGen/discussions
  intended_uses: The primary use of MusicGen is research on AI-based music generation
  license: MIT
  modality:
    explanation: text; audio
    value: audio, text; audio, text
  model_card: https://github.com/facebookresearch/audiocraft/blob/main/model_cards/MUSICGEN_MODEL_CARD.md
  monitoring: ''
  name: MusicGen
  nationality: USA
  organization: Meta
  prohibited_uses: The model should not be used on downstream applications without
    further risk evaluation and mitigation. The model should not be used to intentionally
    create or disseminate music pieces that create hostile or alienating environments
    for people. This includes generating music that people would foreseeably find
    disturbing, distressing, or offensive; or content that propagates historical or
    current stereotypes.
  quality_control: ''
  size: 3.3B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://huggingface.co/spaces/facebook/MusicGen/tree/main
- access: open
  analysis: Evaluated on Frechet Audio Distance and Kullback-Leibler Divergence as
    well as qualitative studies with human participants.
  created_date: 2023-08-02
  dependencies:
  - AudioSet
  - BBC sound effects
  - AudioCaps
  - Clotho v2
  - VGG-Sound
  - FSD50K
  - Free To Use Sounds
  - Sonniss Game Effects
  - WeSoundEffects
  - Paramount Motion - Odeon Cinematic Sound Effects
  description: AudioGen is an auto-regressive generative model that generates audio
    samples conditioned on text inputs
  feedback: https://huggingface.co/facebook/audiogen-medium/discussions
  intended_uses: The primary use of AudioGen is research on AI-based audio generation.
  license: MIT
  modality:
    explanation: text; audio
    value: audio, text; audio, text
  model_card: https://github.com/facebookresearch/audiocraft/blob/main/model_cards/AUDIOGEN_MODEL_CARD.md
  monitoring: ''
  name: AudioGen
  nationality: USA
  organization: Meta
  prohibited_uses: The model should not be used on downstream applications without
    further risk evaluation and mitigation. The model should not be used to intentionally
    create or disseminate audio pieces that create hostile or alienating environments
    for people. This includes generating audio that people would foreseeably find
    disturbing, distressing, or offensive; or content that propagates historical or
    current stereotypes.
  quality_control: ''
  size: 1.5B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://felixkreuk.github.io/audiogen/paper.pdf
- access: closed
  analysis: Emu significantly outperforms a publicly available state-of-the-art model
    SDXLv1.0 on visual appeal when compared on standard benchmarks.
  created_date: 2023-09-27
  dependencies:
  - CLIP
  - T5
  description: Emu is a pre-trained latent diffusion model on 1.1 billion image-text
    pairs and fine-tuned with only a few thousand carefully selected high-quality
    images.
  feedback: ''
  intended_uses: ''
  license: unknown
  modality: text; image
  model_card: none
  monitoring: ''
  name: Emu
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 1.5B parameters (dense)
  training_emissions: ''
  training_hardware: ''
  training_time: ''
  type: model
  url: https://ai.meta.com/research/publications/emu-enhancing-image-generation-models-using-photogenic-needles-in-a-haystack/
- access: open
  analysis: Evaluated on several code benchmarks like HumanEval and MBPP.
  created_date: 2023-08-24
  dependencies:
  - Llama 2
  description: Code Llama is a collection of pretrained and fine-tuned generative
    text models ranging in scale from 7 billion to 34 billion parameters.
  feedback: https://huggingface.co/allenai/codetulu-2-13b/discussions
  intended_uses: Code Llama and its variants is intended for commercial and research
    use in English and relevant programming languages.
  license: Llama 2
  modality: text; code, text
  model_card: https://huggingface.co/codellama/CodeLlama-34b-hf
  monitoring: ''
  name: Code LLaMA
  nationality: USA
  organization: Meta
  prohibited_uses: Use in any manner that violates applicable laws or regulations
    (including trade compliance laws). Use in languages other than English. Use in
    any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement
    for Code Llama and its variants.
  quality_control: ''
  size: 34B parameters (dense)
  training_emissions: 65.3 tCO2eq
  training_hardware: A100-80GB GPUs
  training_time: 400K GPU hours
  type: model
  url: https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/
- access: closed
  analysis: Analyzed against nearest neighbor model baseline and by extending the
    video length.
  created_date: 2023-11-16
  dependencies:
  - Emu
  - CLIP
  - T5
  description: Emu Video is a text-to-video generation model that factorizes the generation
    into two steps, first generating an image conditioned on the text, and then generating
    a video conditioned on the text and the generated image.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: text; video
  model_card: none
  monitoring: ''
  name: Emu Video
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: 6B parameters (dense)
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://emu-video.metademolab.com/
- access: closed
  analysis: Evaluated on test set of actions in comparison to SoTA image editing models.
  created_date: 2023-11-16
  dependencies:
  - Emu
  - CLIP
  - T5
  description: Emu Edit is a multi-task image editing model which sets state-of-the-art
    results in instruction-based image editing.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: text; image
  model_card: none
  monitoring: ''
  name: Emu Edit
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://emu-edit.metademolab.com/
- access: open
  analysis: Evaluated in comparison to CLIP.
  created_date: 2023-10-02
  dependencies:
  - Common Crawl
  description: MetaCLIP is a more transparent rendition of CLIP that aims to reveal
    CLIP's training data curation methods.
  feedback: none
  intended_uses: ''
  license: CC-BY-NC-4.0
  modality: text; text
  model_card: https://huggingface.co/facebook/metaclip-b32-400m
  monitoring: none
  name: MetaCLIP
  nationality: USA
  organization: Meta
  prohibited_uses: ''
  quality_control: ''
  size: unknown
  training_emissions: unknown
  training_hardware: unknown
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2103.00020.pdf
- access: open
  analysis: The models were evaluated based on their performance on standard benchmarks
    and real-world scenarios. These evaluations were performed using a high-quality
    human evaluation set containing 1,800 prompts covering multiple use cases. The
    models also went through red-teaming for safety, where human experts and automated
    methods were used to generate adversarial prompts to test for problematic responses.
  created_date: 2024-04-18
  dependencies: []
  description: Llama 3 is the third generation of Meta AI's open-source large language
    model. It comes with pretrained and instruction-fine-tuned language models with
    8B and 70B parameters that can support a broad range of use cases.
  feedback: Feedback is encouraged from users to improve the model, but the feedback
    mechanism is not explicitly described.
  intended_uses: Llama 3 is intended for a broad range of use cases, including AI
    assistance, content creation, learning, and analysis.
  license:
    explanation: Can be found at https://github.com/meta-llama/llama3/blob/main/LICENSE
    value: Llama 3
  modality: text; text
  model_card: https://github.com/meta-llama/llama3/blob/main/MODEL_CARD.md
  monitoring: Extensive internal and external performance evaluation and red-teaming
    approach for safety testing.
  name: Llama 3
  nationality: USA
  organization: Meta
  prohibited_uses: unknown
  quality_control: Extensive internal and external testing for safety, and design
    of new trust and safety tools.
  size: 70B parameters
  training_emissions: unknown
  training_hardware: 2 custom-built Meta 24K GPU clusters
  training_time: unknown
  type: model
  url: https://llama.meta.com/llama3/
- access: open
  analysis: Evaluated on a comprehensive range of tasks, including visual question
    answering, image captioning, text generation, image generation, and long-form
    mixed modal generation. Chameleon demonstrates broad and general capabilities,
    including state-of-the-art performance in image captioning tasks, outperforms
    Llama-2 in text-only tasks while being competitive with models such as Mixtral
    8x7B and Gemini-Pro.
  created_date: 2024-05-17
  dependencies: []
  description: Chameleon is a family of early-fusion token-based mixed-modal models
    capable of understanding and generating images and text in any arbitrary sequence.
  feedback: none
  intended_uses: ''
  license: unknown
  modality: image, text; image, text
  model_card: none
  monitoring: ''
  name: Chameleon
  nationality: USA
  organization: Meta FAIR
  prohibited_uses: ''
  quality_control: ''
  size: 34B parameters
  training_emissions: unknown
  training_hardware: Meta's Research Super Cluster (powered by NVIDIA A100 80GB GPUs)
  training_time: unknown
  type: model
  url: https://arxiv.org/pdf/2405.09818
- access: open
  analysis: The model was evaluated on over 150 benchmark datasets that span a wide
    range of languages. An experimental evaluation suggests that the model is competitive
    with leading foundation models across a range of tasks. Also, smaller models of
    Llama 3.1 405B are competitive with closed and open models that have a similar
    number of parameters.
  created_date: 2024-07-23
  dependencies:
  - Unknown
  description: Llama 3.1 405B is the first openly available model that rivals the
    top AI models when it comes to state-of-the-art capabilities in general knowledge,
    steerability, math, tool use, and multilingual translation. With the release of
    the 405B model, the Llama versions support advanced use cases, such as long-form
    text summarization, multilingual conversational agents, and coding assistants.
    It is the largest and most capable openly available foundation model.
  feedback: Unknown
  intended_uses: For advanced use cases, such as long-form text summarization, multilingual
    conversational agents, and coding assistants. May also be useful in the development
    of custom offerings and systems by developers.
  license: Unknown
  modality: text; text
  model_card: https://github.com/meta-llama/llama-models/blob/main/models/llama3_1/MODEL_CARD.md
  monitoring: Unknown
  name: Llama 3.1 405B
  nationality: USA
  organization: Meta AI
  prohibited_uses: Unknown
  quality_control: The development process was focused on keeping the model scalable
    and straightforward. It adopted an iterative post-training procedure, where each
    round uses supervised fine-tuning and direct preference optimization. The model
    also underwent quality assurance and filtering for pre-and post-training data.
  size: 405B parameters (dense)
  training_emissions: Unknown
  training_hardware: Over 16 thousand H100 GPUs
  training_time: Unknown
  type: model
  url: https://ai.meta.com/blog/meta-llama-3-1/
- access:
    explanation: Future versions of the tuned models will be released as we improve
      model safety with community feedback.
    value: open
  analysis: Unknown
  created_date: 2024-12-06
  dependencies: []
  description: The Meta Llama 3.3 multilingual large language model (LLM) is an instruction
    tuned generative model in 70B (text in/text out).
  feedback: Instructions on how to provide feedback or comments on the model can be
    found in the model README.
  intended_uses: Intended for commercial and research use in multiple languages. Instruction
    tuned text only models are intended for assistant-like chat.
  license:
    explanation: A custom commercial license, the Llama 3.3 Community License Agreement
    value: Llama 3.3 Community License Agreement
  modality:
    explanation: The Llama 3.3 instruction tuned text only model is optimized for
      multilingual dialogue use cases.
    value: text; text
  model_card: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
  monitoring: Unknown
  name: Llama 3.3
  nationality: USA
  organization: Meta
  prohibited_uses: Use in any manner that violates applicable laws or regulations
    (including trade compliance laws). Use in any other way that is prohibited by
    the Acceptable Use Policy and Llama 3.3 Community License.
  quality_control: Used "supervised fine-tuning (SFT) and reinforcement learning with
    human feedback (RLHF) to align with human preferences for helpfulness and safety."
  size:
    explanation: The Meta Llama 3.3 multilingual large language model (LLM) is an
      instruction tuned generative model in 70B (text in/text out).
    value: 70B parameters
  training_emissions:
    explanation: Training Greenhouse Gas Emissions Estimated total location-based
      greenhouse gas emissions were 11,390 tons CO2eq for training.
    value: 11,390 tons CO2eq
  training_hardware:
    explanation: Training utilized a cumulative of 39.3M GPU hours of computation
      on H100-80GB (TDP of 700W) type hardware.
    value: H100-80GB (TDP of 700W) type hardware
  training_time:
    explanation: Training utilized a cumulative of 39.3M GPU hours of computation
      on H100-80GB (TDP of 700W) type hardware.
    value: 39.3M GPU hours
  type: model
  url: https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct
