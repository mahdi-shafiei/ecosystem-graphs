- access: open
  analysis: Evaluated on several popular benchmarks and performance in different fields.
  created_date: 2023-10-30
  dependencies:
  - SkyPile
  description: The Skywork series is a family of large language models (LLMs) trained
    on a corpus of over 3.2 trillion tokens drawn from both English and Chinese texts.
  feedback: https://huggingface.co/Skywork/Skywork-13B-base/discussions
  intended_uses: ''
  license:
    explanation: can be found at https://github.com/SkyworkAI/Skywork/blob/main/LICENSE
    value: custom
  modality: text; text
  model_card: https://huggingface.co/Skywork/Skywork-13B-base
  monitoring: none
  name: Skywork
  nationality: China
  organization: Kunlun Inc.
  prohibited_uses: ''
  quality_control: ''
  size: 13B parameters (dense)
  training_emissions: unknown
  training_hardware: 512 A800-80GB GPUs
  training_time: 39 days
  type: model
  url: https://arxiv.org/pdf/2310.19341.pdf
