- access:
    explanation: 'Model training code can be accessed at the official Dolly repository.
      Trained weights can be requested at hello-dolly@databricks.com. [[Dolly Repository]](https://github.com/databrickslabs/dolly).

      '
    value: open
  analysis: '"We evaluated Dolly on the instruction-following capabilities described
    in the InstructGPT paper that ChatGPT is based on and found that it exhibits many
    of the same qualitative capabilities, including text generation, brainstorming
    and open Q&A." [[Databricks Blog Post]] (https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).

    '
  created_date:
    explanation: 'The date the model was announced in the [[Cerebras blog post]](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html).

      '
    value: 2023-03-24
  dependencies:
  - GPT-J
  - Alpaca dataset
  description: "\"Databricks\u2019 Dolly, a large language model trained on the Databricks\n\
    \ Machine Learning Platform, demonstrates that a two-years-old open source\n model\
    \ (GPT-J) can, when subjected to just 30 minutes of fine tuning on a\n focused\
    \ corpus of 50k records (Stanford Alpaca), exhibit surprisingly\n high quality\
    \ instruction following behavior not characteristic of the\n foundation model\
    \ on which it is based.\"\n [[Dolly Repository]](https://github.com/databrickslabs/dolly).\n"
  feedback: https://github.com/databrickslabs/dolly/issues
  intended_uses: '"Dolly is intended exclusively for research purposes and is not
    licensed for commercial use." [[Limitations]](https://github.com/databrickslabs/dolly#limitations).

    '
  license: Apache 2.0
  modality: text; text
  model_card: ''
  monitoring: none
  name: Dolly
  nationality: USA
  organization: Databricks
  prohibited_uses: 'Authors note the following limitations of the model: "The Dolly
    model family is under active development, and so any list of shortcomings is unlikely
    to be exhaustive, but we include known limitations and misfires here as a means
    to document and share our preliminary findings with the community. In particular,
    dolly-6b struggles with syntactically complex prompts, mathematical operations,
    factual errors, dates and times, open-ended question answering, hallucination,
    enumerating lists of specific length, and stylistic mimicry." [[Limitations]](https://github.com/databrickslabs/dolly#limitations).

    '
  quality_control: none
  size: 6B parameters (dense)
  training_emissions: unknown
  training_hardware:
    explanation: 'According to [[Model Overview]](https://github.com/databrickslabs/dolly#model-overview)

      '
    value: A single NDasrA100_v4 machine with 8x A100 40GB GPUs
  training_time:
    explanation: 'According to [[Model Overview]](https://github.com/databrickslabs/dolly#model-overview)

      '
    value: 30 minutes
  type: model
  url: https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html
- access: open
  analysis: DBRX outperforms established open-source and open-weight base models on
    the Databricks Model Gauntlet, the Hugging Face Open LLM Leaderboard, and HumanEval.
    Full evaluation details can be found in the corresponding technical blog post.
  created_date: 2024-03-27
  dependencies: []
  description: DBRX is a transformer-based decoder-only large language model (LLM)
    that was trained using next-token prediction by Databricks. It uses a fine-grained
    mixture-of-experts (MoE) architecture with 132B total parameters of which 36B
    parameters are active on any input. DBRX only accepts text-based inputs and produces
    text-based outputs.
  feedback: https://huggingface.co/databricks/dbrx-base/discussions
  intended_uses: DBRX models are open, general-purpose LLMs intended and licensed
    for both commercial and research applications. They can be further fine-tuned
    for various domain-specific natural language and coding tasks.
  license: Databricks Open Model License
  modality: text; text
  model_card: https://huggingface.co/databricks/dbrx-base
  monitoring: unknown
  name: DBRX
  nationality: USA
  organization: Databricks
  prohibited_uses: DBRX models are not intended to be used out-of-the-box in non-English
    languages, and do not support native code execution, function calling or any use
    that violates applicable laws or regulations or is otherwise prohibited by the
    Databricks Open Model License and Databricks Open Model Acceptable Use Policy.
  quality_control: Recommendations provided for retrieval augmented generation (RAG)
    in scenarios where accuracy and fidelity are important and additional testing
    around safety in the context of the specific application and domain is suggested.
  size: 132B parameters (sparse)
  training_emissions: unknown
  training_hardware: 3072 NVIDIA H100s connected by 3.2Tbps Infiniband
  training_time: 3 months
  type: model
  url: https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm
